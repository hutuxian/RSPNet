grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
/usr/local/python378-gcc540/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:26: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  def convert_to_list(value, n, name, dtype=np.int):
argument type:  <class 'str'>
Working tree is dirty. Patch:
diff --git a/config/finetune/ucf101_resnet18.jsonnet b/config/finetune/ucf101_resnet18.jsonnet
index 09f5f8a..61e2e4d 100644
--- a/config/finetune/ucf101_resnet18.jsonnet
+++ b/config/finetune/ucf101_resnet18.jsonnet
@@ -19,5 +19,5 @@ default {
         batch_size: 16 * batch_size_factor,
     },
     optimizer+: {lr: 0.1},
-    num_epochs: 30,
+    num_epochs: 2,
 }
diff --git a/config/optimizer/loss_lambda.libsonnet b/config/optimizer/loss_lambda.libsonnet
index d5da363..5f79fd6 100644
--- a/config/optimizer/loss_lambda.libsonnet
+++ b/config/optimizer/loss_lambda.libsonnet
@@ -1,5 +1,5 @@
 {
     A: 1.0,
-    M: 1.0,
+    M: 0.2,
     F: 1.0,
-}
\ No newline at end of file
+}
diff --git a/config/pretrain/moco-train-base.jsonnet b/config/pretrain/moco-train-base.jsonnet
index 252faac..fb9301f 100644
--- a/config/pretrain/moco-train-base.jsonnet
+++ b/config/pretrain/moco-train-base.jsonnet
@@ -10,7 +10,7 @@ local loss_lambda = import "../optimizer/loss_lambda.libsonnet";
         arch: $.arch,
     },
 
-    dataset: kinetics100, // or kinetics100
+    dataset: kinetics100, // or kinetics400
 
     batch_size: 64,
     num_workers: 4,
diff --git a/config/pretrain/s3dg.jsonnet b/config/pretrain/s3dg.jsonnet
index bf6e94a..f93c786 100644
--- a/config/pretrain/s3dg.jsonnet
+++ b/config/pretrain/s3dg.jsonnet
@@ -1,7 +1,7 @@
 local base = import "moco-train-base.jsonnet";
 
 base {
-    batch_size: 64,
+    batch_size: 32,
     num_workers: 4,
 
     arch: 's3dg',
diff --git a/datasets/classification/__init__.py b/datasets/classification/__init__.py
index be5201b..3485633 100644
--- a/datasets/classification/__init__.py
+++ b/datasets/classification/__init__.py
@@ -140,10 +140,11 @@ class DataLoaderFactoryV3:
         dl = DataLoader(
             video_dataset,
             #batch_size=batch_size,
-            #num_workers=self.cfg.get_int('num_workers'),
-            num_workers=0,
+            num_workers=self.cfg.get_int('num_workers'),
+            #num_workers=0,
             batch_sampler=sampler,
-            # drop_last=(split == 'train'),
+            use_shared_memory =False,
+            #drop_last=(split == 'train'),
             #collate_fn=identity,  # We will deal with collation on main thread.
             collate_fn=gpu_collate_fn,  # We will deal with collation on main thread.
             # multiprocessing_context=mp.get_context('fork'),
diff --git a/datasets/classification/video.py b/datasets/classification/video.py
index fdd79bf..6d22ca3 100644
--- a/datasets/classification/video.py
+++ b/datasets/classification/video.py
@@ -73,11 +73,11 @@ class VideoDataset(Dataset):
         clip_frame_indices_list = [self.temporal_transform(frame_indices) for _ in range(self.num_clips_per_sample)]
         # Fetch all frames in one `vr.get_batch` call
         clip_frame_indices = np.concatenate(clip_frame_indices_list)  # [a1, a2, ..., an, b1, b2, ...,bn]
+        decord.bridge.set_bridge('torch')
         clips: torch.Tensor = vr.get_batch(clip_frame_indices)  # [N*T, H, W, C]
         clip_list = clips.chunk(len(clip_frame_indices_list), dim=0)  # List[Tensor[T, H, W, C]]
-
+        
         clip_list = [self.spatial_transform(clip) for clip in clip_list]
-
         return clip_list, sample.class_index
 
     def __len__(self):
diff --git a/datasets/transforms_video/transforms_tensor.py b/datasets/transforms_video/transforms_tensor.py
index d008074..4f63db6 100644
--- a/datasets/transforms_video/transforms_tensor.py
+++ b/datasets/transforms_video/transforms_tensor.py
@@ -217,7 +217,7 @@ class SequentialGPUCollateFn:
 
         if self.target_transform:
             label_tensor = torch.as_tensor(label)
-            label_tensor = label_tensor.cuda(device=self.device, non_blocking=True)
+            #label_tensor = label_tensor.cuda(device=self.device, non_blocking=True)
         else:
             label_tensor = None
 
@@ -226,10 +226,9 @@ class SequentialGPUCollateFn:
         clip_list: List[List[Optional[Tensor]]] = [[None for _ in range(batch_size)] for _ in range(num_clips)]
         for batch_index, clip in enumerate(clips):  # batch of clip0, clip1
             for clip_index, clip_tensor in enumerate(clip):
-                clip_tensor = clip_tensor.cuda(device=self.device, non_blocking=True)
+                #clip_tensor = clip_tensor.cuda(device=self.device, non_blocking=True)
                 clip_tensor = self.transform(clip_tensor)
                 clip_list[clip_index][batch_index] = clip_tensor
-
         clip_list = [np.array(torch.stack(x, dim=0).cpu()) for x in clip_list]
-        # return (clip_list, label_tensor, *others)
-        return clip_list
+        return (clip_list, label_tensor.cpu().numpy() if label_tensor is not None else np.zeros_like(clip_list[0]), others)
+        #return clip_list
diff --git a/finetune.py b/finetune.py
index 2a47321..a85ea75 100644
--- a/finetune.py
+++ b/finetune.py
@@ -2,11 +2,12 @@ import time
 import logging
 import warnings
 
-import torch
-import torch.distributed as dist
-import torch.multiprocessing as mp
+import paddle
+import paddle.distributed as dist
 from pyhocon import ConfigTree
-from torch import nn
+from paddle import nn
+from paddle.distributed import fleet
+
 from torch.utils.tensorboard import SummaryWriter
 
 from arguments import Args
@@ -423,7 +424,9 @@ class Engine:
             self.summary_writer.flush()
 
 
-def main_worker(local_rank: int, args: Args, dist_url: str):
+def main_worker(args: Args):
+    
+    
     print('Local Rank:', local_rank)
 
     # log in main process only
@@ -485,18 +488,8 @@ def main():
     pack_code(args.run_dir)
 
     utils.environment.ulimit_n_max()
-
-    free_port = utils.distributed.find_free_port()
-    dist_url = f'tcp://127.0.0.1:{free_port}'
-
-    print(f'world_size={args.world_size} Using dist_url={dist_url}')
-
-    """
-    We only consider single node here. 'world_size' is the number of processes.
-    """
-    args.parser = None
-    mp.spawn(main_worker, args=(args, dist_url,), nprocs=args.world_size)
-
+    main_worker(args)
+    
 
 if __name__ == '__main__':
     main()
diff --git a/framework/arguments.py b/framework/arguments.py
index 68f2955..883187a 100644
--- a/framework/arguments.py
+++ b/framework/arguments.py
@@ -77,8 +77,13 @@ class Args(TypedArgs):
         if self.experiment_dir is not None:
             self.experiment_dir.mkdir(parents=True, exist_ok=True)
             if not self.ask_for_replacing_older_dir(self.run_dir):
-                raise EnvironmentError(f'Run dir "{self.run_dir}" exists')
-            self.run_dir.mkdir(parents=True, exist_ok=False)
+                print(f'Run dir "{self.run_dir}" exists')
+                #raise EnvironmentError(f'Run dir "{self.run_dir}" exists')
+            else:
+                try:
+                    self.run_dir.mkdir(parents=True, exist_ok=False)
+                except Exception as e:
+                    print(f'Run dir "{self.run_dir}" exists#################')
 
     def make_experiment_dir(self):
         if not self.ask_for_replacing_older_dir(self.experiment_dir):
@@ -92,9 +97,10 @@ class Args(TypedArgs):
         print(
             f'File exists: {dir_to_be_replaced}\nDo you want to remove it and create a new one?'
         )
-        choice = input('Remove older directory? [y]es/[n]o: ')
-
-        if choice in ['y', 'yes']:
-            shutil.rmtree(dir_to_be_replaced)
-            return True
-        return False
+        #choice = input('Remove older directory? [y]es/[n]o: ')
+        shutil.rmtree(dir_to_be_replaced)
+        return True
+        #if choice in ['y', 'yes']:
+        #    shutil.rmtree(dir_to_be_replaced)
+        #    return True
+        #return False
diff --git a/moco/split_wrapper.py b/moco/split_wrapper.py
index cdb4660..1a981d4 100644
--- a/moco/split_wrapper.py
+++ b/moco/split_wrapper.py
@@ -2,34 +2,33 @@ import functools
 import logging
 from typing import *
 
-import torch
-import torch.nn.functional as F
-from torch import Tensor, nn
+import paddle
+import paddle.nn.functional as F
+from paddle import Tensor, nn
 
 logger = logging.getLogger(__name__)
 
 
-class Flatten(nn.Module):
-
-    def forward(self, x: Tensor):
+class Flatten(nn.Layer):
+    def forward(self, x):
         return x.flatten(1)
 
 
-class ConvFc(nn.Module):
+class ConvFc(nn.Layer):
     """
     conv->relu->conv->downsample->linear
 
     """
 
-    def __init__(self, feat_dim: int, moco_dim: int, kernel_size: Tuple[int, int, int], padding: Tuple[int, int, int]):
+    def __init__(self, feat_dim, moco_dim, kernel_size, padding):
         super().__init__()
-        self.conv1 = nn.Conv3d(feat_dim, feat_dim, kernel_size, padding=padding)
+        self.conv1 = nn.Conv3D(feat_dim, feat_dim, kernel_size, padding=padding)
         self.relu = nn.ReLU(inplace=True)
-        self.conv2 = nn.Conv3d(feat_dim, feat_dim, kernel_size, padding=padding)
-        self.avg_pool = nn.AdaptiveAvgPool3d((1, 1, 1))
+        self.conv2 = nn.Conv3D(feat_dim, feat_dim, kernel_size, padding=padding)
+        self.avg_pool = nn.AdaptiveAvgPool3D((1, 1, 1))
         self.linear = nn.Linear(feat_dim, moco_dim)
 
-    def forward(self, x: Tensor):
+    def forward(self, x):
         out = self.conv1(x)
         out = self.relu(out)
         out = self.conv2(out)
@@ -39,21 +38,21 @@ class ConvFc(nn.Module):
         return out
 
 
-class ConvBnFc(nn.Module):
+class ConvBnFc(nn.Layer):
     """
     conv->relu->conv->downsample->linear
 
     """
 
-    def __init__(self, feat_dim: int, moco_dim: int, kernel_size: Tuple[int, int, int], padding: Tuple[int, int, int]):
+    def __init__(self, feat_dim, moco_dim, kernel_size, padding):
         super().__init__()
-        self.conv1 = nn.Conv3d(feat_dim, feat_dim, kernel_size, padding=padding)
-        self.bn = nn.BatchNorm3d(feat_dim)
+        self.conv1 = nn.Conv3D(feat_dim, feat_dim, kernel_size, padding=padding)
+        self.bn = nn.BatchNorm3D(feat_dim)
         self.relu = nn.ReLU(inplace=True)
-        self.avg_pool = nn.AdaptiveAvgPool3d((1, 1, 1))
+        self.avg_pool = nn.AdaptiveAvgPool3D((1, 1, 1))
         self.linear = nn.Linear(feat_dim, moco_dim)
 
-    def forward(self, x: Tensor):
+    def forward(self, x):
         out = self.conv1(x)
         out = self.bn(out)
         out = self.relu(out)
@@ -63,28 +62,27 @@ class ConvBnFc(nn.Module):
         return out
 
 
-class MultiTaskWrapper(nn.Module):
+class MultiTaskWrapper(nn.Layer):
     """
     This wrapper adds two independent projection layers (one for each pretext task) behind the backbone network.
     The projection layer type can be linear layer and mlp (as indicated in SimCLR).
     """
     def __init__(
             self,
-            base_encoder: Callable[[int], nn.Module],
-            num_classes: int = 128,
-            finetune: bool = False,
-            fc_type: str = 'linear',
-            groups: int = 1,
+            base_encoder,
+            num_classes = 128,
+            finetune = False,
+            fc_type = 'linear',
+            groups = 1,
     ):
         """
-
         :param base_encoder:
         :param num_classes:
         :param finetune:
         :param fc_type:
         :param groups:
         """
-        super().__init__()
+        super(MultiTaskWrapper, self).__init__()
 
         logger.info('Using MultiTask Wrapper')
         self.finetune = finetune
@@ -93,7 +91,7 @@ class MultiTaskWrapper(nn.Module):
         self.groups = groups
         self.fc_type = fc_type
 
-        logger.warning(f'{self.__class__} using groups: {groups}')
+        # logger.warning(f'{self.__class__} using groups: {groups}')
 
         self.encoder = base_encoder(num_classes=1)
 
@@ -101,7 +99,7 @@ class MultiTaskWrapper(nn.Module):
         feat_dim //= groups
 
         if self.finetune:
-            self.avg_pool = nn.AdaptiveAvgPool3d((1, 1, 1))
+            self.avg_pool = nn.AdaptiveAvgPool3D((1, 1, 1))
             self.fc = nn.Linear(feat_dim, num_classes)
         else:
             if fc_type == 'linear':
@@ -124,12 +122,16 @@ class MultiTaskWrapper(nn.Module):
                 self.fc1 = self._get_linear_fc(feat_dim, self.moco_dim)
                 self.fc2 = self._get_linear_fc(feat_dim, 1)  # use for speed binary classification like speednet
 
-    def forward(self, x: Tensor):
-        feat: Tensor = self.encoder.get_feature(x)
-
+    def forward(self, x):
+        print("==========enter==========")
+        feat = self.encoder.get_feature(x)
+        print("==============forward========",feat.shape)
+        
         if self.finetune:
             x3 = self.avg_pool(feat)
+            print("=============MMMMM",x3.shape)
             x3 = x3.flatten(1)
+            print("=========VVVVV===========")
             x3 = self.fc(x3)
             return x3
         else:
@@ -142,26 +144,26 @@ class MultiTaskWrapper(nn.Module):
                 x2 = self.fc2(feat2)
             else:
                 raise Exception
-            x1 = F.normalize(x1, dim=1)
+            x1 = F.normalize(x1, axis=1)
 
             if self.fc_type == 'speednet':  # for speednet, it use sigmoid to ouput the probability that whether the clip is sped up 
-                x2 = torch.sigmoid(x2)
+                x2 = paddle.sigmoid(x2)
             else:
-                x2 = F.normalize(x2, dim=1)
+                x2 = F.normalize(x2, axis=1)
             return x1, x2
 
     @staticmethod
-    def _get_linear_fc(feat_dim: int, moco_dim: int):
+    def _get_linear_fc(feat_dim, moco_dim):
         return nn.Sequential(
-            nn.AdaptiveAvgPool3d((1, 1, 1)),
+            nn.AdaptiveAvgPool3D((1, 1, 1)),
             Flatten(),
             nn.Linear(feat_dim, moco_dim),
         )
 
     @staticmethod
-    def _get_mlp_fc(feat_dim: int, moco_dim: int):
+    def _get_mlp_fc(feat_dim, moco_dim):
         return nn.Sequential(
-            nn.AdaptiveAvgPool3d((1, 1, 1)),
+            nn.AdaptiveAvgPool3D((1, 1, 1)),
             Flatten(),
             nn.Linear(feat_dim, feat_dim),
             nn.ReLU(inplace=True),
@@ -174,7 +176,7 @@ class MultiTaskWrapper(nn.Module):
         feat_dim = 512
         for fc_name in fc_names:
             if hasattr(encoder, fc_name):
-                feat_dim = getattr(encoder, fc_name).in_features
-                logger.info(f'Found fc: {fc_name} with in_features: {feat_dim}')
+                feat_dim = getattr(encoder, fc_name).weight.shape[0]
+                logger.warning('Found fc: %s with in_features: %d' % (fc_name, feat_dim))
                 break
         return feat_dim
diff --git a/model_paddle.py b/model_paddle.py
index cf7c6a9..338b70f 100644
--- a/model_paddle.py
+++ b/model_paddle.py
@@ -189,15 +189,15 @@ class MoCoDiffLossTwoFc(nn.Layer):
     @paddle.no_grad()
     def _forward_encoder_k(self, im_k):
         # shuffle for making use of BN
-        im_k, idx_unshuffle = self._batch_shuffle_ddp(im_k)
+        #im_k, idx_unshuffle = self._batch_shuffle_ddp(im_k)
 
         k_A, k_M = self.encoder_k(im_k)  # keys: NxC
         # k_A = self.encoder_q(im_k)
         # k_M = self.encoder_q(im_k)
 
         # undo shuffle
-        k_A = self._batch_unshuffle_ddp(k_A, idx_unshuffle)
-        k_M = self._batch_unshuffle_ddp(k_M, idx_unshuffle)
+        #k_A = self._batch_unshuffle_ddp(k_A, idx_unshuffle)
+        #k_M = self._batch_unshuffle_ddp(k_M, idx_unshuffle)
 
         return k_A, k_M
 
@@ -305,6 +305,7 @@ def concat_all_gather(tensor):
 
     tensors_gather = [paddle.ones_like(tensor)
         for _ in range(paddle.distributed.get_world_size())]
+    tensors_gather=[]
     paddle.distributed.all_gather(tensors_gather, tensor)
 
     output = paddle.concat(tensors_gather, axis=0)
diff --git a/models/__init__.py b/models/__init__.py
index 0474b23..02c0bef 100644
--- a/models/__init__.py
+++ b/models/__init__.py
@@ -4,16 +4,19 @@ Abstract the process of creating a model into get_model_class, model_class(num_c
 """
 
 from pyhocon import ConfigTree, ConfigFactory
-from torch import nn
-import torch
-from torch import nn
+#from torch import nn
+#import torch
+#from torch import nn
 from typing import *
 import logging
 
+import paddle
+from paddle import nn
+
 logger = logging.getLogger(__name__)
 
 
-def get_model_class(**kwargs) -> Callable[[int], nn.Module]:
+def get_model_class(**kwargs) -> Callable[[int], nn.Layer]:
     """
     Pass the model config as parameters. For convinence, we change the cfg to dict, and then reverse it
     :param kwargs:
@@ -80,8 +83,9 @@ class ModelFactory:
     def __init__(self, cfg: ConfigTree):
         self.cfg = cfg
 
-    def _post_process_model(self, model: nn.Module):
+    def _post_process_model(self, model: nn.Layer):
         if self.cfg.get_bool('only_train_fc', False):
+            print("===============fix some layer===============>")
             for param in model.parameters():
                 param.requires_grad = False
 
@@ -105,7 +109,7 @@ class ModelFactory:
 
         return model
 
-    def build(self, local_rank: int) -> nn.Module:
+    def build(self, local_rank: int) -> nn.Layer:
         # arch = self.cfg.get_string('model.arch')
         num_classes = self.cfg.get_int('dataset.num_classes')
 
@@ -114,30 +118,30 @@ class ModelFactory:
         model = model_class(num_classes=num_classes)
         model = self._post_process_model(model)
 
-        model = model.cuda(local_rank)
-
-        model = nn.parallel.DistributedDataParallel(
-            model,
-            device_ids=[local_rank],
-        )
+        #model = model.cuda(local_rank)
+        #model = nn.parallel.DistributedDataParallel(
+        #    model,
+        #    device_ids=[local_rank],
+        #)
         return model
 
-    def build_multitask_wrapper(self, local_rank: int) -> nn.Module:
+    def build_multitask_wrapper(self, local_rank: int) -> nn.Layer:
         # arch = self.cfg.get_string('model.arch')
 
         from moco.split_wrapper import MultiTaskWrapper
         num_classes = self.cfg.get_int('dataset.num_classes')
 
         model_class = get_model_class(**self.cfg.get_config('model'))
-
+       
+        print("============model_class============>",model_class)
+        
         model = MultiTaskWrapper(model_class, num_classes=num_classes, finetune=True)
         model = self._post_process_model(model)
 
-        model = model.cuda(local_rank)
-
-        model = nn.parallel.DistributedDataParallel(
-            model,
-            device_ids=[local_rank],
-            find_unused_parameters=True,  # some of forward output are not involved in calculation
-        )
+        #model = model.cuda(local_rank)
+        #model = nn.parallel.DistributedDataParallel(
+        #    model,
+        #    device_ids=[local_rank],
+        #    find_unused_parameters=True,  # some of forward output are not involved in calculation
+        #)
         return model
diff --git a/models/resnet.py b/models/resnet.py
index 6f0b294..347059a 100644
--- a/models/resnet.py
+++ b/models/resnet.py
@@ -6,9 +6,10 @@ Commit id: 4e2195c
 import math
 from functools import partial
 
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
+import paddle
+import paddle.nn as nn
+import paddle.nn.functional as F
+import numpy as np
 
 __all__ = [
     'ResNet', 'resnet10', 'resnet18', 'resnet34', 'resnet50', 'resnet101',
@@ -18,43 +19,43 @@ __all__ = [
 
 def conv3x3x3(in_planes, out_planes, stride=1):
     # 3x3x3 convolution with padding
-    return nn.Conv3d(
+    return nn.Conv3D(
         in_planes,
         out_planes,
         kernel_size=3,
         stride=stride,
         padding=1,
-        bias=False)
+        bias_attr=False)
 
 
 def downsample_basic_block(x, planes, stride):
-    out = F.avg_pool3d(x, kernel_size=1, stride=stride)
-    # zero_pads = torch.Tensor(
+    out = F.avg_pool3D(x, kernel_size=1, stride=stride)
+    # zero_pads = paddle.Tensor(
     #     out.size(0), planes - out.size(1), out.size(2), out.size(3),
     #     out.size(4)).zero_()
-    # if isinstance(out.data, torch.cuda.FloatTensor):
+    # if isinstance(out.data, paddle.cuda.FloatTensor):
     #     zero_pads = zero_pads.cuda()
-    zero_pads = torch.zeros(
+    zero_pads = paddle.zeros(
         out.shape[0], planes - out.shape[1], out.shape[2], out.shape[3],
         device=x.device
     )
 
-    # out = Variable(torch.cat([out.data, zero_pads], dim=1))
-    out = torch.cat([out.data, zero_pads], dim=1)
+    # out = Variable(paddle.cat([out.data, zero_pads], dim=1))
+    out = paddle.cat([out.data, zero_pads], dim=1)
 
     return out
 
 
-class BasicBlock(nn.Module):
+class BasicBlock(nn.Layer):
     expansion = 1
 
     def __init__(self, inplanes, planes, stride=1, downsample=None):
         super(BasicBlock, self).__init__()
         self.conv1 = conv3x3x3(inplanes, planes, stride)
-        self.bn1 = nn.BatchNorm3d(planes)
-        self.relu = nn.ReLU(inplace=True)
+        self.bn1 = nn.BatchNorm3D(planes)
+        self.relu = nn.ReLU()
         self.conv2 = conv3x3x3(planes, planes)
-        self.bn2 = nn.BatchNorm3d(planes)
+        self.bn2 = nn.BatchNorm3D(planes)
         self.downsample = downsample
         self.stride = stride
 
@@ -77,19 +78,19 @@ class BasicBlock(nn.Module):
         return out
 
 
-class Bottleneck(nn.Module):
+class Bottleneck(nn.Layer):
     expansion = 4
 
     def __init__(self, inplanes, planes, stride=1, downsample=None):
         super(Bottleneck, self).__init__()
-        self.conv1 = nn.Conv3d(inplanes, planes, kernel_size=1, bias=False)
-        self.bn1 = nn.BatchNorm3d(planes)
-        self.conv2 = nn.Conv3d(
-            planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
-        self.bn2 = nn.BatchNorm3d(planes)
-        self.conv3 = nn.Conv3d(planes, planes * 4, kernel_size=1, bias=False)
-        self.bn3 = nn.BatchNorm3d(planes * 4)
-        self.relu = nn.ReLU(inplace=True)
+        self.conv1 = nn.Conv3D(inplanes, planes, kernel_size=1, bias_attr=False)
+        self.bn1 = nn.BatchNorm3D(planes)
+        self.conv2 = nn.Conv3D(
+            planes, planes, kernel_size=3, stride=stride, padding=1, bias_attr=False)
+        self.bn2 = nn.BatchNorm3D(planes)
+        self.conv3 = nn.Conv3D(planes, planes * 4, kernel_size=1, bias_attr=False)
+        self.bn3 = nn.BatchNorm3D(planes * 4)
+        self.relu = nn.ReLU()
         self.downsample = downsample
         self.stride = stride
 
@@ -116,7 +117,7 @@ class Bottleneck(nn.Module):
         return out
 
 
-class ResNet(nn.Module):
+class ResNet(nn.Layer):
 
     def __init__(self,
                  block,
@@ -127,16 +128,16 @@ class ResNet(nn.Module):
                  num_classes=400):
         self.inplanes = 64
         super(ResNet, self).__init__()
-        self.conv1 = nn.Conv3d(
+        self.conv1 = nn.Conv3D(
             3,
             64,
             kernel_size=7,
             stride=(1, 2, 2),
             padding=(3, 3, 3),
-            bias=False)
-        self.bn1 = nn.BatchNorm3d(64)
-        self.relu = nn.ReLU(inplace=True)
-        self.maxpool = nn.MaxPool3d(kernel_size=(3, 3, 3), stride=2, padding=1)
+            bias_attr=False)
+        self.bn1 = nn.BatchNorm3D(64)
+        self.relu = nn.ReLU()
+        self.maxpool = nn.MaxPool3D(kernel_size=(3, 3, 3), stride=2, padding=1)
         self.layer1 = self._make_layer(block, 64, layers[0], shortcut_type)
         self.layer2 = self._make_layer(
             block, 128, layers[1], shortcut_type, stride=2)
@@ -146,16 +147,24 @@ class ResNet(nn.Module):
             block, 512, layers[3], shortcut_type, stride=2)
         last_duration = int(math.ceil(sample_duration / 16))
         last_size = int(math.ceil(sample_size / 32))
-        self.avgpool = nn.AvgPool3d(
+        self.avgpool = nn.AvgPool3D(
             (last_duration, last_size, last_size), stride=1)
         self.fc = nn.Linear(512 * block.expansion, num_classes)
-
-        for m in self.modules():
-            if isinstance(m, nn.Conv3d):
-                m.weight = nn.init.kaiming_normal_(m.weight, mode='fan_out')
-            elif isinstance(m, nn.BatchNorm3d):
-                m.weight.data.fill_(1)
-                m.bias.data.zero_()
+        # self.fc = nn.Linear(512 * 4, num_classes)
+
+        # for m in self.modules():
+        # for m in self.sublayers():
+        #     if isinstance(m, nn.Conv3D):
+        #         pass
+        #         # m.weight = nn.init.kaiming_normal_(m.weight, mode='fan_out')
+        #         # print("m.weight", m.weight)
+        #     elif isinstance(m, nn.BatchNorm3D):
+        #         print("type(m.weight)", type(m.weight))
+        #         print("before m.weight", m.weight)
+        #         # m.weight.data.fill_(1)
+        #         # m.bias.data.zero_()
+        #         m.weight = paddle.zeros_like(m.weight)
+        #         print("after m.weight", m.weight)
 
     def _make_layer(self, block, planes, blocks, shortcut_type, stride=1):
         downsample = None
@@ -167,12 +176,12 @@ class ResNet(nn.Module):
                     stride=stride)
             else:
                 downsample = nn.Sequential(
-                    nn.Conv3d(
+                    nn.Conv3D(
                         self.inplanes,
                         planes * block.expansion,
                         kernel_size=1,
                         stride=stride,
-                        bias=False), nn.BatchNorm3d(planes * block.expansion))
+                        bias_attr=False), nn.BatchNorm3D(planes * block.expansion))
 
         layers = []
         layers.append(block(self.inplanes, planes, stride, downsample))
@@ -187,17 +196,16 @@ class ResNet(nn.Module):
         x = self.bn1(x)
         x = self.relu(x)
         x = self.maxpool(x)
-
+        
         x = self.layer1(x)
         x = self.layer2(x)
         x = self.layer3(x)
         x = self.layer4(x)
 
         x = self.avgpool(x)
-
-        x = x.view(x.size(0), -1)
+        x = paddle.reshape(x, shape=[paddle.shape(x)[0], -1])
         x = self.fc(x)
-
+        
         return x
 
     def get_feature(self, x):
@@ -217,7 +225,7 @@ class ResNet(nn.Module):
 
         x = self.avgpool(feat)
 
-        x = x.view(x.size(0), -1)
+        x = paddle.reshape(x, shape=[paddle.shape(x)[0], -1])
         x = self.fc(x)
 
         return x, feat
@@ -294,8 +302,8 @@ def resnet200(**kwargs):
 
 
 if __name__ == '__main__':
-    x = torch.rand(2, 3, 16, 112, 112)
+    x = paddle.randn([2, 3, 16, 112, 112])
     m = resnet18()
-    print(m)
+    # print(m)
     y = m(x)
-    print(y.shape)
+    # print(y.shape)
diff --git a/pretrain.py b/pretrain.py
index f9feda9..0696980 100644
--- a/pretrain.py
+++ b/pretrain.py
@@ -24,7 +24,7 @@ from framework.utils.checkpoint import CheckpointManager
 from moco import ModelFactory
 from moco.builder_diffspeed_diffloss import Loss
 from utils.moco import replace_moco_k_in_config
-
+import logging
 logger = logging.getLogger(__name__)
 
 
@@ -51,7 +51,14 @@ class Engine:
             A=self.loss_lambda.get_float('A'),
             M=self.loss_lambda.get_float('M')
         )
-
+        
+        #whc add a logger
+        self.logger=logging.getLogger()
+        self.logger.setLevel(logging.INFO)
+        fh=logging.FileHandler("test_log/torch_res.log",mode="w")
+        fh.setLevel(logging.INFO)
+        self.logger.addHandler(fh)
+        
         self.train_loader = self.data_loader_factory.build(vid=True, device=self.device)
 
         self.learning_rate = self.cfg.get_float('optimizer.lr')
@@ -173,7 +180,9 @@ class Engine:
             batch_size = len(clip_q)
             self.top1_meter_A_n.update(acc1_A_n, batch_size)
             self.top5_meter_A_n.update(acc5_A_n, batch_size)
-
+            
+            #whc
+            self.logger.info("whc {} {} {} {} {} {} {} {}".format(self.current_epoch,i,loss.detach().numpy()[0], loss_A.detach().numpy()[0], loss_M.detach().numpy()[0],acc1_A.detach().numpy()[0], acc5_A.detach().numpy()[0], acc1_M.detach().numpy()[0]))
             if i > 0 and i % self.log_interval == 0:
                 # Do logging as late as possible. this will force CUDA sync.
                 # Log numbers from last iteration, just before update
diff --git a/train_paddle.py b/train_paddle.py
index 142f2b7..d7fadcb 100644
--- a/train_paddle.py
+++ b/train_paddle.py
@@ -6,14 +6,17 @@ import paddle.distributed.spawn as spawn
 import numpy as np
 import model_paddle as model
 import models.resnet as resnet
-
+import os
 from arguments import Args
 from framework import utils
-
 from datasets.classification import DataLoaderFactoryV3
 from framework.config import get_config, save_config
+import paddle.distributed.fleet.base.role_maker as role_maker
+#from framework.utils.whc_checkpoint import CheckpointManager
 from paddle.distributed import fleet
-
+#whc
+import logging
+import time
 
 class Engine:
     def __init__(self, args, cfg, local_rank):
@@ -22,13 +25,34 @@ class Engine:
         self.current_epoch = 0
         # self.batch_size = self.args.train_batch_size
         self.batch_size = cfg.get_int('batch_size')
-        self.batch_size = 64
-
-        fleet.init(is_collective=True)
+        #self.batch_size = 64
+        self.local_rank=local_rank
+        self.best_loss = float('inf')
+        self.arch = cfg.get_string('arch')
+        role = role_maker.PaddleCloudRoleMaker(is_collective=True)
+        fleet.init(role)
+       
+        self.device = paddle.CUDAPlace(fleet.worker_index()) 
+        print("========================>",fleet.worker_index()) 
+        if fleet.worker_index()==0:
+            #whc add a logger
+            self.logger=logging.getLogger()
+            self.logger.setLevel(logging.INFO)
+            fh=logging.FileHandler("test_log/paddle_res.log",mode="w")
+            fh.setLevel(logging.INFO)
+            self.logger.addHandler(fh)
+            #self.checkpoint = CheckpointManager(
+            #    args.experiment_dir,
+            #    keep_interval=cfg.get_int('checkpoint_interval')
+            #)
+        else:
+            self.checkpoint = None
+
+        #fleet.init(is_collective=True)
         self.model = model.MoCoWrapper(cfg).build_moco_diffloss()
 
         self.data_loader_factory = DataLoaderFactoryV3(cfg)
-        self.train_loader = self.data_loader_factory.build(vid=True)
+        self.train_loader = self.data_loader_factory.build(vid=True,device=None)
 
         self.learning_rate = cfg.get_float('optimizer.lr')
         self.loss_lambda = cfg.get_config('loss_lambda')
@@ -37,10 +61,17 @@ class Engine:
             A=self.loss_lambda.get_float('A'),
             M=self.loss_lambda.get_float('M')
         )
-
+        print("####################world_size=",self.args.world_size)
+        if not self.args.no_scale_lr:
+            self.learning_rate = utils.environment.scale_learning_rate(
+                self.learning_rate,
+                self.args.world_size if self.args.world_size==8 else 8,
+                self.batch_size,
+            ) 
+        self.scheduler = paddle.optimizer.lr.CosineAnnealingDecay(learning_rate=self.learning_rate,T_max=self.epochs,eta_min=self.learning_rate/1000)
         self.optimizer = paddle.optimizer.Momentum(
             parameters=self.model.parameters(),
-            learning_rate=self.learning_rate,
+            learning_rate=self.scheduler,
             momentum=cfg.get_float('optimizer.momentum'),
             # dampening=cfg.get_float('optimizer.dampening'),
             weight_decay=cfg.get_float('optimizer.weight_decay'),
@@ -57,7 +88,11 @@ class Engine:
         clip_q = paddle.randn([self.batch_size, 3, 16, 112, 112])
         clip_k = paddle.randn([self.batch_size, 3, 16, 112, 112])
         # for i in range(loop):
-        for i, (clip_q, clip_k) in enumerate(self.train_loader):
+        avg_loss=0
+        nn=0
+        for i, (clip_list, label_tensor, others)  in enumerate(self.train_loader):
+            clip_q, clip_k = clip_list
+            clip_q, clip_k=paddle.to_tensor(clip_q,place=self.device),paddle.to_tensor(clip_k,place=self.device)
             output, target, ranking_logits, ranking_target = self.model(clip_q, clip_k)
             loss, loss_A, loss_M = self.criterion(output, target, ranking_logits, ranking_target)
 
@@ -76,10 +111,29 @@ class Engine:
             # acc1_A_n = accuracy(output[1], target, k=1)
             # acc5_A_n = accuracy(output[1], target, k=5)
 
+            #whc get the avg_loss
+            avg_loss += loss.detach().numpy()[0]
+            nn += 1
+
             # if i > 0 and i % self.log_interval == 0:
             if True:
                 print("loss: %f, loss_A: %f, loss_M: %f" % (loss, loss_A, loss_M))
                 print("acc1_A: %f, acc5_A: %f, acc1_M: %f" % (acc1_A, acc5_A, acc1_M))
+                if fleet.worker_index()==0:
+                    self.logger.info("whc {} {} {} {} {} {} {} {}".format(self.current_epoch,i,loss.detach().numpy()[0], loss_A.detach().numpy()[0], loss_M.detach().numpy()[0],acc1_A.detach().numpy()[0], acc5_A.detach().numpy()[0], acc1_M.detach().numpy()[0]))
+
+        if fleet.worker_index()== 0:
+            avg_loss = avg_loss / nn
+            is_best = avg_loss < self.best_loss
+            self.best_loss = min(avg_loss,self.best_loss)
+            if is_best:
+                best_param=os.path.join(str(self.args.experiment_dir),"best_mode.pdparams")
+                best_optim=os.path.join(str(self.args.experiment_dir),"best_optim.pdparams")
+                best_sche=os.path.join(str(self.args.experiment_dir),"best_sche.pdparams")
+                paddle.save(self.model.state_dict(),best_param)
+                paddle.save(self.optimizer.state_dict(),best_optim)
+                paddle.save(self.scheduler.state_dict(),best_sche) 
+ 
                 # Do logging as late as possible. this will force CUDA sync.
                 # Log numbers from last iteration, just before update
                 # logger.info(
@@ -89,12 +143,14 @@ class Engine:
                 #     f'{self.top1_meter_A_n}\t{self.top5_meter_A_n}'
                 # )
 
-
             
     def run(self):
         while self.current_epoch < self.epochs:
             print("train epoch: %d" % (self.current_epoch))
             self.train_epoch()
+            if fleet.worker_index()==0:
+                self.logger.info("epoch {} {}".format(self.current_epoch,self.scheduler.get_lr()))
+            self.scheduler.step()
             self.current_epoch += 1
 
 

Local Rank: 0
I0310 00:50:20.627038   905 nccl_context.cc:189] init nccl context nranks: 2 local rank: 1 gpu id: 1 ring id: 0
W0310 00:50:21.250638   905 device_context.cc:362] Please NOTE: device: 1, GPU Compute Capability: 7.0, Driver API Version: 11.0, Runtime API Version: 10.0
W0310 00:50:21.254895   905 device_context.cc:372] device: 1, cuDNN Version: 7.6.
grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
/usr/local/python378-gcc540/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:26: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  def convert_to_list(value, n, name, dtype=np.int):
argument type:  <class 'str'>
Working tree is dirty. Patch:
diff --git a/config/finetune/ucf101_resnet18.jsonnet b/config/finetune/ucf101_resnet18.jsonnet
index 09f5f8a..61e2e4d 100644
--- a/config/finetune/ucf101_resnet18.jsonnet
+++ b/config/finetune/ucf101_resnet18.jsonnet
@@ -19,5 +19,5 @@ default {
         batch_size: 16 * batch_size_factor,
     },
     optimizer+: {lr: 0.1},
-    num_epochs: 30,
+    num_epochs: 2,
 }
diff --git a/config/optimizer/loss_lambda.libsonnet b/config/optimizer/loss_lambda.libsonnet
index d5da363..5f79fd6 100644
--- a/config/optimizer/loss_lambda.libsonnet
+++ b/config/optimizer/loss_lambda.libsonnet
@@ -1,5 +1,5 @@
 {
     A: 1.0,
-    M: 1.0,
+    M: 0.2,
     F: 1.0,
-}
\ No newline at end of file
+}
diff --git a/config/pretrain/moco-train-base.jsonnet b/config/pretrain/moco-train-base.jsonnet
index 252faac..fb9301f 100644
--- a/config/pretrain/moco-train-base.jsonnet
+++ b/config/pretrain/moco-train-base.jsonnet
@@ -10,7 +10,7 @@ local loss_lambda = import "../optimizer/loss_lambda.libsonnet";
         arch: $.arch,
     },
 
-    dataset: kinetics100, // or kinetics100
+    dataset: kinetics100, // or kinetics400
 
     batch_size: 64,
     num_workers: 4,
diff --git a/config/pretrain/s3dg.jsonnet b/config/pretrain/s3dg.jsonnet
index bf6e94a..f93c786 100644
--- a/config/pretrain/s3dg.jsonnet
+++ b/config/pretrain/s3dg.jsonnet
@@ -1,7 +1,7 @@
 local base = import "moco-train-base.jsonnet";
 
 base {
-    batch_size: 64,
+    batch_size: 32,
     num_workers: 4,
 
     arch: 's3dg',
diff --git a/datasets/classification/__init__.py b/datasets/classification/__init__.py
index be5201b..3485633 100644
--- a/datasets/classification/__init__.py
+++ b/datasets/classification/__init__.py
@@ -140,10 +140,11 @@ class DataLoaderFactoryV3:
         dl = DataLoader(
             video_dataset,
             #batch_size=batch_size,
-            #num_workers=self.cfg.get_int('num_workers'),
-            num_workers=0,
+            num_workers=self.cfg.get_int('num_workers'),
+            #num_workers=0,
             batch_sampler=sampler,
-            # drop_last=(split == 'train'),
+            use_shared_memory =False,
+            #drop_last=(split == 'train'),
             #collate_fn=identity,  # We will deal with collation on main thread.
             collate_fn=gpu_collate_fn,  # We will deal with collation on main thread.
             # multiprocessing_context=mp.get_context('fork'),
diff --git a/datasets/classification/video.py b/datasets/classification/video.py
index fdd79bf..6d22ca3 100644
--- a/datasets/classification/video.py
+++ b/datasets/classification/video.py
@@ -73,11 +73,11 @@ class VideoDataset(Dataset):
         clip_frame_indices_list = [self.temporal_transform(frame_indices) for _ in range(self.num_clips_per_sample)]
         # Fetch all frames in one `vr.get_batch` call
         clip_frame_indices = np.concatenate(clip_frame_indices_list)  # [a1, a2, ..., an, b1, b2, ...,bn]
+        decord.bridge.set_bridge('torch')
         clips: torch.Tensor = vr.get_batch(clip_frame_indices)  # [N*T, H, W, C]
         clip_list = clips.chunk(len(clip_frame_indices_list), dim=0)  # List[Tensor[T, H, W, C]]
-
+        
         clip_list = [self.spatial_transform(clip) for clip in clip_list]
-
         return clip_list, sample.class_index
 
     def __len__(self):
diff --git a/datasets/transforms_video/transforms_tensor.py b/datasets/transforms_video/transforms_tensor.py
index d008074..4f63db6 100644
--- a/datasets/transforms_video/transforms_tensor.py
+++ b/datasets/transforms_video/transforms_tensor.py
@@ -217,7 +217,7 @@ class SequentialGPUCollateFn:
 
         if self.target_transform:
             label_tensor = torch.as_tensor(label)
-            label_tensor = label_tensor.cuda(device=self.device, non_blocking=True)
+            #label_tensor = label_tensor.cuda(device=self.device, non_blocking=True)
         else:
             label_tensor = None
 
@@ -226,10 +226,9 @@ class SequentialGPUCollateFn:
         clip_list: List[List[Optional[Tensor]]] = [[None for _ in range(batch_size)] for _ in range(num_clips)]
         for batch_index, clip in enumerate(clips):  # batch of clip0, clip1
             for clip_index, clip_tensor in enumerate(clip):
-                clip_tensor = clip_tensor.cuda(device=self.device, non_blocking=True)
+                #clip_tensor = clip_tensor.cuda(device=self.device, non_blocking=True)
                 clip_tensor = self.transform(clip_tensor)
                 clip_list[clip_index][batch_index] = clip_tensor
-
         clip_list = [np.array(torch.stack(x, dim=0).cpu()) for x in clip_list]
-        # return (clip_list, label_tensor, *others)
-        return clip_list
+        return (clip_list, label_tensor.cpu().numpy() if label_tensor is not None else np.zeros_like(clip_list[0]), others)
+        #return clip_list
diff --git a/finetune.py b/finetune.py
index 2a47321..a85ea75 100644
--- a/finetune.py
+++ b/finetune.py
@@ -2,11 +2,12 @@ import time
 import logging
 import warnings
 
-import torch
-import torch.distributed as dist
-import torch.multiprocessing as mp
+import paddle
+import paddle.distributed as dist
 from pyhocon import ConfigTree
-from torch import nn
+from paddle import nn
+from paddle.distributed import fleet
+
 from torch.utils.tensorboard import SummaryWriter
 
 from arguments import Args
@@ -423,7 +424,9 @@ class Engine:
             self.summary_writer.flush()
 
 
-def main_worker(local_rank: int, args: Args, dist_url: str):
+def main_worker(args: Args):
+    
+    
     print('Local Rank:', local_rank)
 
     # log in main process only
@@ -485,18 +488,8 @@ def main():
     pack_code(args.run_dir)
 
     utils.environment.ulimit_n_max()
-
-    free_port = utils.distributed.find_free_port()
-    dist_url = f'tcp://127.0.0.1:{free_port}'
-
-    print(f'world_size={args.world_size} Using dist_url={dist_url}')
-
-    """
-    We only consider single node here. 'world_size' is the number of processes.
-    """
-    args.parser = None
-    mp.spawn(main_worker, args=(args, dist_url,), nprocs=args.world_size)
-
+    main_worker(args)
+    
 
 if __name__ == '__main__':
     main()
diff --git a/framework/arguments.py b/framework/arguments.py
index 68f2955..883187a 100644
--- a/framework/arguments.py
+++ b/framework/arguments.py
@@ -77,8 +77,13 @@ class Args(TypedArgs):
         if self.experiment_dir is not None:
             self.experiment_dir.mkdir(parents=True, exist_ok=True)
             if not self.ask_for_replacing_older_dir(self.run_dir):
-                raise EnvironmentError(f'Run dir "{self.run_dir}" exists')
-            self.run_dir.mkdir(parents=True, exist_ok=False)
+                print(f'Run dir "{self.run_dir}" exists')
+                #raise EnvironmentError(f'Run dir "{self.run_dir}" exists')
+            else:
+                try:
+                    self.run_dir.mkdir(parents=True, exist_ok=False)
+                except Exception as e:
+                    print(f'Run dir "{self.run_dir}" exists#################')
 
     def make_experiment_dir(self):
         if not self.ask_for_replacing_older_dir(self.experiment_dir):
@@ -92,9 +97,10 @@ class Args(TypedArgs):
         print(
             f'File exists: {dir_to_be_replaced}\nDo you want to remove it and create a new one?'
         )
-        choice = input('Remove older directory? [y]es/[n]o: ')
-
-        if choice in ['y', 'yes']:
-            shutil.rmtree(dir_to_be_replaced)
-            return True
-        return False
+        #choice = input('Remove older directory? [y]es/[n]o: ')
+        shutil.rmtree(dir_to_be_replaced)
+        return True
+        #if choice in ['y', 'yes']:
+        #    shutil.rmtree(dir_to_be_replaced)
+        #    return True
+        #return False
diff --git a/moco/split_wrapper.py b/moco/split_wrapper.py
index cdb4660..1a981d4 100644
--- a/moco/split_wrapper.py
+++ b/moco/split_wrapper.py
@@ -2,34 +2,33 @@ import functools
 import logging
 from typing import *
 
-import torch
-import torch.nn.functional as F
-from torch import Tensor, nn
+import paddle
+import paddle.nn.functional as F
+from paddle import Tensor, nn
 
 logger = logging.getLogger(__name__)
 
 
-class Flatten(nn.Module):
-
-    def forward(self, x: Tensor):
+class Flatten(nn.Layer):
+    def forward(self, x):
         return x.flatten(1)
 
 
-class ConvFc(nn.Module):
+class ConvFc(nn.Layer):
     """
     conv->relu->conv->downsample->linear
 
     """
 
-    def __init__(self, feat_dim: int, moco_dim: int, kernel_size: Tuple[int, int, int], padding: Tuple[int, int, int]):
+    def __init__(self, feat_dim, moco_dim, kernel_size, padding):
         super().__init__()
-        self.conv1 = nn.Conv3d(feat_dim, feat_dim, kernel_size, padding=padding)
+        self.conv1 = nn.Conv3D(feat_dim, feat_dim, kernel_size, padding=padding)
         self.relu = nn.ReLU(inplace=True)
-        self.conv2 = nn.Conv3d(feat_dim, feat_dim, kernel_size, padding=padding)
-        self.avg_pool = nn.AdaptiveAvgPool3d((1, 1, 1))
+        self.conv2 = nn.Conv3D(feat_dim, feat_dim, kernel_size, padding=padding)
+        self.avg_pool = nn.AdaptiveAvgPool3D((1, 1, 1))
         self.linear = nn.Linear(feat_dim, moco_dim)
 
-    def forward(self, x: Tensor):
+    def forward(self, x):
         out = self.conv1(x)
         out = self.relu(out)
         out = self.conv2(out)
@@ -39,21 +38,21 @@ class ConvFc(nn.Module):
         return out
 
 
-class ConvBnFc(nn.Module):
+class ConvBnFc(nn.Layer):
     """
     conv->relu->conv->downsample->linear
 
     """
 
-    def __init__(self, feat_dim: int, moco_dim: int, kernel_size: Tuple[int, int, int], padding: Tuple[int, int, int]):
+    def __init__(self, feat_dim, moco_dim, kernel_size, padding):
         super().__init__()
-        self.conv1 = nn.Conv3d(feat_dim, feat_dim, kernel_size, padding=padding)
-        self.bn = nn.BatchNorm3d(feat_dim)
+        self.conv1 = nn.Conv3D(feat_dim, feat_dim, kernel_size, padding=padding)
+        self.bn = nn.BatchNorm3D(feat_dim)
         self.relu = nn.ReLU(inplace=True)
-        self.avg_pool = nn.AdaptiveAvgPool3d((1, 1, 1))
+        self.avg_pool = nn.AdaptiveAvgPool3D((1, 1, 1))
         self.linear = nn.Linear(feat_dim, moco_dim)
 
-    def forward(self, x: Tensor):
+    def forward(self, x):
         out = self.conv1(x)
         out = self.bn(out)
         out = self.relu(out)
@@ -63,28 +62,27 @@ class ConvBnFc(nn.Module):
         return out
 
 
-class MultiTaskWrapper(nn.Module):
+class MultiTaskWrapper(nn.Layer):
     """
     This wrapper adds two independent projection layers (one for each pretext task) behind the backbone network.
     The projection layer type can be linear layer and mlp (as indicated in SimCLR).
     """
     def __init__(
             self,
-            base_encoder: Callable[[int], nn.Module],
-            num_classes: int = 128,
-            finetune: bool = False,
-            fc_type: str = 'linear',
-            groups: int = 1,
+            base_encoder,
+            num_classes = 128,
+            finetune = False,
+            fc_type = 'linear',
+            groups = 1,
     ):
         """
-
         :param base_encoder:
         :param num_classes:
         :param finetune:
         :param fc_type:
         :param groups:
         """
-        super().__init__()
+        super(MultiTaskWrapper, self).__init__()
 
         logger.info('Using MultiTask Wrapper')
         self.finetune = finetune
@@ -93,7 +91,7 @@ class MultiTaskWrapper(nn.Module):
         self.groups = groups
         self.fc_type = fc_type
 
-        logger.warning(f'{self.__class__} using groups: {groups}')
+        # logger.warning(f'{self.__class__} using groups: {groups}')
 
         self.encoder = base_encoder(num_classes=1)
 
@@ -101,7 +99,7 @@ class MultiTaskWrapper(nn.Module):
         feat_dim //= groups
 
         if self.finetune:
-            self.avg_pool = nn.AdaptiveAvgPool3d((1, 1, 1))
+            self.avg_pool = nn.AdaptiveAvgPool3D((1, 1, 1))
             self.fc = nn.Linear(feat_dim, num_classes)
         else:
             if fc_type == 'linear':
@@ -124,12 +122,16 @@ class MultiTaskWrapper(nn.Module):
                 self.fc1 = self._get_linear_fc(feat_dim, self.moco_dim)
                 self.fc2 = self._get_linear_fc(feat_dim, 1)  # use for speed binary classification like speednet
 
-    def forward(self, x: Tensor):
-        feat: Tensor = self.encoder.get_feature(x)
-
+    def forward(self, x):
+        print("==========enter==========")
+        feat = self.encoder.get_feature(x)
+        print("==============forward========",feat.shape)
+        
         if self.finetune:
             x3 = self.avg_pool(feat)
+            print("=============MMMMM",x3.shape)
             x3 = x3.flatten(1)
+            print("=========VVVVV===========")
             x3 = self.fc(x3)
             return x3
         else:
@@ -142,26 +144,26 @@ class MultiTaskWrapper(nn.Module):
                 x2 = self.fc2(feat2)
             else:
                 raise Exception
-            x1 = F.normalize(x1, dim=1)
+            x1 = F.normalize(x1, axis=1)
 
             if self.fc_type == 'speednet':  # for speednet, it use sigmoid to ouput the probability that whether the clip is sped up 
-                x2 = torch.sigmoid(x2)
+                x2 = paddle.sigmoid(x2)
             else:
-                x2 = F.normalize(x2, dim=1)
+                x2 = F.normalize(x2, axis=1)
             return x1, x2
 
     @staticmethod
-    def _get_linear_fc(feat_dim: int, moco_dim: int):
+    def _get_linear_fc(feat_dim, moco_dim):
         return nn.Sequential(
-            nn.AdaptiveAvgPool3d((1, 1, 1)),
+            nn.AdaptiveAvgPool3D((1, 1, 1)),
             Flatten(),
             nn.Linear(feat_dim, moco_dim),
         )
 
     @staticmethod
-    def _get_mlp_fc(feat_dim: int, moco_dim: int):
+    def _get_mlp_fc(feat_dim, moco_dim):
         return nn.Sequential(
-            nn.AdaptiveAvgPool3d((1, 1, 1)),
+            nn.AdaptiveAvgPool3D((1, 1, 1)),
             Flatten(),
             nn.Linear(feat_dim, feat_dim),
             nn.ReLU(inplace=True),
@@ -174,7 +176,7 @@ class MultiTaskWrapper(nn.Module):
         feat_dim = 512
         for fc_name in fc_names:
             if hasattr(encoder, fc_name):
-                feat_dim = getattr(encoder, fc_name).in_features
-                logger.info(f'Found fc: {fc_name} with in_features: {feat_dim}')
+                feat_dim = getattr(encoder, fc_name).weight.shape[0]
+                logger.warning('Found fc: %s with in_features: %d' % (fc_name, feat_dim))
                 break
         return feat_dim
diff --git a/model_paddle.py b/model_paddle.py
index cf7c6a9..338b70f 100644
--- a/model_paddle.py
+++ b/model_paddle.py
@@ -189,15 +189,15 @@ class MoCoDiffLossTwoFc(nn.Layer):
     @paddle.no_grad()
     def _forward_encoder_k(self, im_k):
         # shuffle for making use of BN
-        im_k, idx_unshuffle = self._batch_shuffle_ddp(im_k)
+        #im_k, idx_unshuffle = self._batch_shuffle_ddp(im_k)
 
         k_A, k_M = self.encoder_k(im_k)  # keys: NxC
         # k_A = self.encoder_q(im_k)
         # k_M = self.encoder_q(im_k)
 
         # undo shuffle
-        k_A = self._batch_unshuffle_ddp(k_A, idx_unshuffle)
-        k_M = self._batch_unshuffle_ddp(k_M, idx_unshuffle)
+        #k_A = self._batch_unshuffle_ddp(k_A, idx_unshuffle)
+        #k_M = self._batch_unshuffle_ddp(k_M, idx_unshuffle)
 
         return k_A, k_M
 
@@ -305,6 +305,7 @@ def concat_all_gather(tensor):
 
     tensors_gather = [paddle.ones_like(tensor)
         for _ in range(paddle.distributed.get_world_size())]
+    tensors_gather=[]
     paddle.distributed.all_gather(tensors_gather, tensor)
 
     output = paddle.concat(tensors_gather, axis=0)
diff --git a/models/__init__.py b/models/__init__.py
index 0474b23..02c0bef 100644
--- a/models/__init__.py
+++ b/models/__init__.py
@@ -4,16 +4,19 @@ Abstract the process of creating a model into get_model_class, model_class(num_c
 """
 
 from pyhocon import ConfigTree, ConfigFactory
-from torch import nn
-import torch
-from torch import nn
+#from torch import nn
+#import torch
+#from torch import nn
 from typing import *
 import logging
 
+import paddle
+from paddle import nn
+
 logger = logging.getLogger(__name__)
 
 
-def get_model_class(**kwargs) -> Callable[[int], nn.Module]:
+def get_model_class(**kwargs) -> Callable[[int], nn.Layer]:
     """
     Pass the model config as parameters. For convinence, we change the cfg to dict, and then reverse it
     :param kwargs:
@@ -80,8 +83,9 @@ class ModelFactory:
     def __init__(self, cfg: ConfigTree):
         self.cfg = cfg
 
-    def _post_process_model(self, model: nn.Module):
+    def _post_process_model(self, model: nn.Layer):
         if self.cfg.get_bool('only_train_fc', False):
+            print("===============fix some layer===============>")
             for param in model.parameters():
                 param.requires_grad = False
 
@@ -105,7 +109,7 @@ class ModelFactory:
 
         return model
 
-    def build(self, local_rank: int) -> nn.Module:
+    def build(self, local_rank: int) -> nn.Layer:
         # arch = self.cfg.get_string('model.arch')
         num_classes = self.cfg.get_int('dataset.num_classes')
 
@@ -114,30 +118,30 @@ class ModelFactory:
         model = model_class(num_classes=num_classes)
         model = self._post_process_model(model)
 
-        model = model.cuda(local_rank)
-
-        model = nn.parallel.DistributedDataParallel(
-            model,
-            device_ids=[local_rank],
-        )
+        #model = model.cuda(local_rank)
+        #model = nn.parallel.DistributedDataParallel(
+        #    model,
+        #    device_ids=[local_rank],
+        #)
         return model
 
-    def build_multitask_wrapper(self, local_rank: int) -> nn.Module:
+    def build_multitask_wrapper(self, local_rank: int) -> nn.Layer:
         # arch = self.cfg.get_string('model.arch')
 
         from moco.split_wrapper import MultiTaskWrapper
         num_classes = self.cfg.get_int('dataset.num_classes')
 
         model_class = get_model_class(**self.cfg.get_config('model'))
-
+       
+        print("============model_class============>",model_class)
+        
         model = MultiTaskWrapper(model_class, num_classes=num_classes, finetune=True)
         model = self._post_process_model(model)
 
-        model = model.cuda(local_rank)
-
-        model = nn.parallel.DistributedDataParallel(
-            model,
-            device_ids=[local_rank],
-            find_unused_parameters=True,  # some of forward output are not involved in calculation
-        )
+        #model = model.cuda(local_rank)
+        #model = nn.parallel.DistributedDataParallel(
+        #    model,
+        #    device_ids=[local_rank],
+        #    find_unused_parameters=True,  # some of forward output are not involved in calculation
+        #)
         return model
diff --git a/models/resnet.py b/models/resnet.py
index 6f0b294..347059a 100644
--- a/models/resnet.py
+++ b/models/resnet.py
@@ -6,9 +6,10 @@ Commit id: 4e2195c
 import math
 from functools import partial
 
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
+import paddle
+import paddle.nn as nn
+import paddle.nn.functional as F
+import numpy as np
 
 __all__ = [
     'ResNet', 'resnet10', 'resnet18', 'resnet34', 'resnet50', 'resnet101',
@@ -18,43 +19,43 @@ __all__ = [
 
 def conv3x3x3(in_planes, out_planes, stride=1):
     # 3x3x3 convolution with padding
-    return nn.Conv3d(
+    return nn.Conv3D(
         in_planes,
         out_planes,
         kernel_size=3,
         stride=stride,
         padding=1,
-        bias=False)
+        bias_attr=False)
 
 
 def downsample_basic_block(x, planes, stride):
-    out = F.avg_pool3d(x, kernel_size=1, stride=stride)
-    # zero_pads = torch.Tensor(
+    out = F.avg_pool3D(x, kernel_size=1, stride=stride)
+    # zero_pads = paddle.Tensor(
     #     out.size(0), planes - out.size(1), out.size(2), out.size(3),
     #     out.size(4)).zero_()
-    # if isinstance(out.data, torch.cuda.FloatTensor):
+    # if isinstance(out.data, paddle.cuda.FloatTensor):
     #     zero_pads = zero_pads.cuda()
-    zero_pads = torch.zeros(
+    zero_pads = paddle.zeros(
         out.shape[0], planes - out.shape[1], out.shape[2], out.shape[3],
         device=x.device
     )
 
-    # out = Variable(torch.cat([out.data, zero_pads], dim=1))
-    out = torch.cat([out.data, zero_pads], dim=1)
+    # out = Variable(paddle.cat([out.data, zero_pads], dim=1))
+    out = paddle.cat([out.data, zero_pads], dim=1)
 
     return out
 
 
-class BasicBlock(nn.Module):
+class BasicBlock(nn.Layer):
     expansion = 1
 
     def __init__(self, inplanes, planes, stride=1, downsample=None):
         super(BasicBlock, self).__init__()
         self.conv1 = conv3x3x3(inplanes, planes, stride)
-        self.bn1 = nn.BatchNorm3d(planes)
-        self.relu = nn.ReLU(inplace=True)
+        self.bn1 = nn.BatchNorm3D(planes)
+        self.relu = nn.ReLU()
         self.conv2 = conv3x3x3(planes, planes)
-        self.bn2 = nn.BatchNorm3d(planes)
+        self.bn2 = nn.BatchNorm3D(planes)
         self.downsample = downsample
         self.stride = stride
 
@@ -77,19 +78,19 @@ class BasicBlock(nn.Module):
         return out
 
 
-class Bottleneck(nn.Module):
+class Bottleneck(nn.Layer):
     expansion = 4
 
     def __init__(self, inplanes, planes, stride=1, downsample=None):
         super(Bottleneck, self).__init__()
-        self.conv1 = nn.Conv3d(inplanes, planes, kernel_size=1, bias=False)
-        self.bn1 = nn.BatchNorm3d(planes)
-        self.conv2 = nn.Conv3d(
-            planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
-        self.bn2 = nn.BatchNorm3d(planes)
-        self.conv3 = nn.Conv3d(planes, planes * 4, kernel_size=1, bias=False)
-        self.bn3 = nn.BatchNorm3d(planes * 4)
-        self.relu = nn.ReLU(inplace=True)
+        self.conv1 = nn.Conv3D(inplanes, planes, kernel_size=1, bias_attr=False)
+        self.bn1 = nn.BatchNorm3D(planes)
+        self.conv2 = nn.Conv3D(
+            planes, planes, kernel_size=3, stride=stride, padding=1, bias_attr=False)
+        self.bn2 = nn.BatchNorm3D(planes)
+        self.conv3 = nn.Conv3D(planes, planes * 4, kernel_size=1, bias_attr=False)
+        self.bn3 = nn.BatchNorm3D(planes * 4)
+        self.relu = nn.ReLU()
         self.downsample = downsample
         self.stride = stride
 
@@ -116,7 +117,7 @@ class Bottleneck(nn.Module):
         return out
 
 
-class ResNet(nn.Module):
+class ResNet(nn.Layer):
 
     def __init__(self,
                  block,
@@ -127,16 +128,16 @@ class ResNet(nn.Module):
                  num_classes=400):
         self.inplanes = 64
         super(ResNet, self).__init__()
-        self.conv1 = nn.Conv3d(
+        self.conv1 = nn.Conv3D(
             3,
             64,
             kernel_size=7,
             stride=(1, 2, 2),
             padding=(3, 3, 3),
-            bias=False)
-        self.bn1 = nn.BatchNorm3d(64)
-        self.relu = nn.ReLU(inplace=True)
-        self.maxpool = nn.MaxPool3d(kernel_size=(3, 3, 3), stride=2, padding=1)
+            bias_attr=False)
+        self.bn1 = nn.BatchNorm3D(64)
+        self.relu = nn.ReLU()
+        self.maxpool = nn.MaxPool3D(kernel_size=(3, 3, 3), stride=2, padding=1)
         self.layer1 = self._make_layer(block, 64, layers[0], shortcut_type)
         self.layer2 = self._make_layer(
             block, 128, layers[1], shortcut_type, stride=2)
@@ -146,16 +147,24 @@ class ResNet(nn.Module):
             block, 512, layers[3], shortcut_type, stride=2)
         last_duration = int(math.ceil(sample_duration / 16))
         last_size = int(math.ceil(sample_size / 32))
-        self.avgpool = nn.AvgPool3d(
+        self.avgpool = nn.AvgPool3D(
             (last_duration, last_size, last_size), stride=1)
         self.fc = nn.Linear(512 * block.expansion, num_classes)
-
-        for m in self.modules():
-            if isinstance(m, nn.Conv3d):
-                m.weight = nn.init.kaiming_normal_(m.weight, mode='fan_out')
-            elif isinstance(m, nn.BatchNorm3d):
-                m.weight.data.fill_(1)
-                m.bias.data.zero_()
+        # self.fc = nn.Linear(512 * 4, num_classes)
+
+        # for m in self.modules():
+        # for m in self.sublayers():
+        #     if isinstance(m, nn.Conv3D):
+        #         pass
+        #         # m.weight = nn.init.kaiming_normal_(m.weight, mode='fan_out')
+        #         # print("m.weight", m.weight)
+        #     elif isinstance(m, nn.BatchNorm3D):
+        #         print("type(m.weight)", type(m.weight))
+        #         print("before m.weight", m.weight)
+        #         # m.weight.data.fill_(1)
+        #         # m.bias.data.zero_()
+        #         m.weight = paddle.zeros_like(m.weight)
+        #         print("after m.weight", m.weight)
 
     def _make_layer(self, block, planes, blocks, shortcut_type, stride=1):
         downsample = None
@@ -167,12 +176,12 @@ class ResNet(nn.Module):
                     stride=stride)
             else:
                 downsample = nn.Sequential(
-                    nn.Conv3d(
+                    nn.Conv3D(
                         self.inplanes,
                         planes * block.expansion,
                         kernel_size=1,
                         stride=stride,
-                        bias=False), nn.BatchNorm3d(planes * block.expansion))
+                        bias_attr=False), nn.BatchNorm3D(planes * block.expansion))
 
         layers = []
         layers.append(block(self.inplanes, planes, stride, downsample))
@@ -187,17 +196,16 @@ class ResNet(nn.Module):
         x = self.bn1(x)
         x = self.relu(x)
         x = self.maxpool(x)
-
+        
         x = self.layer1(x)
         x = self.layer2(x)
         x = self.layer3(x)
         x = self.layer4(x)
 
         x = self.avgpool(x)
-
-        x = x.view(x.size(0), -1)
+        x = paddle.reshape(x, shape=[paddle.shape(x)[0], -1])
         x = self.fc(x)
-
+        
         return x
 
     def get_feature(self, x):
@@ -217,7 +225,7 @@ class ResNet(nn.Module):
 
         x = self.avgpool(feat)
 
-        x = x.view(x.size(0), -1)
+        x = paddle.reshape(x, shape=[paddle.shape(x)[0], -1])
         x = self.fc(x)
 
         return x, feat
@@ -294,8 +302,8 @@ def resnet200(**kwargs):
 
 
 if __name__ == '__main__':
-    x = torch.rand(2, 3, 16, 112, 112)
+    x = paddle.randn([2, 3, 16, 112, 112])
     m = resnet18()
-    print(m)
+    # print(m)
     y = m(x)
-    print(y.shape)
+    # print(y.shape)
diff --git a/pretrain.py b/pretrain.py
index f9feda9..0696980 100644
--- a/pretrain.py
+++ b/pretrain.py
@@ -24,7 +24,7 @@ from framework.utils.checkpoint import CheckpointManager
 from moco import ModelFactory
 from moco.builder_diffspeed_diffloss import Loss
 from utils.moco import replace_moco_k_in_config
-
+import logging
 logger = logging.getLogger(__name__)
 
 
@@ -51,7 +51,14 @@ class Engine:
             A=self.loss_lambda.get_float('A'),
             M=self.loss_lambda.get_float('M')
         )
-
+        
+        #whc add a logger
+        self.logger=logging.getLogger()
+        self.logger.setLevel(logging.INFO)
+        fh=logging.FileHandler("test_log/torch_res.log",mode="w")
+        fh.setLevel(logging.INFO)
+        self.logger.addHandler(fh)
+        
         self.train_loader = self.data_loader_factory.build(vid=True, device=self.device)
 
         self.learning_rate = self.cfg.get_float('optimizer.lr')
@@ -173,7 +180,9 @@ class Engine:
             batch_size = len(clip_q)
             self.top1_meter_A_n.update(acc1_A_n, batch_size)
             self.top5_meter_A_n.update(acc5_A_n, batch_size)
-
+            
+            #whc
+            self.logger.info("whc {} {} {} {} {} {} {} {}".format(self.current_epoch,i,loss.detach().numpy()[0], loss_A.detach().numpy()[0], loss_M.detach().numpy()[0],acc1_A.detach().numpy()[0], acc5_A.detach().numpy()[0], acc1_M.detach().numpy()[0]))
             if i > 0 and i % self.log_interval == 0:
                 # Do logging as late as possible. this will force CUDA sync.
                 # Log numbers from last iteration, just before update
diff --git a/train_paddle.py b/train_paddle.py
index 142f2b7..f1bc1a8 100644
--- a/train_paddle.py
+++ b/train_paddle.py
@@ -6,14 +6,18 @@ import paddle.distributed.spawn as spawn
 import numpy as np
 import model_paddle as model
 import models.resnet as resnet
-
+import os
 from arguments import Args
 from framework import utils
-
 from datasets.classification import DataLoaderFactoryV3
 from framework.config import get_config, save_config
+import paddle.distributed.fleet.base.role_maker as role_maker
+#from framework.utils.whc_checkpoint import CheckpointManager
 from paddle.distributed import fleet
-
+#whc
+import logging
+import time
+import datetime
 
 class Engine:
     def __init__(self, args, cfg, local_rank):
@@ -22,13 +26,34 @@ class Engine:
         self.current_epoch = 0
         # self.batch_size = self.args.train_batch_size
         self.batch_size = cfg.get_int('batch_size')
-        self.batch_size = 64
-
-        fleet.init(is_collective=True)
+        #self.batch_size = 64
+        self.local_rank=local_rank
+        self.best_loss = float('inf')
+        self.arch = cfg.get_string('arch')
+        role = role_maker.PaddleCloudRoleMaker(is_collective=True)
+        fleet.init(role)
+       
+        self.device = paddle.CUDAPlace(fleet.worker_index()) 
+        print("========================>",fleet.worker_index()) 
+        if fleet.worker_index()==0:
+            #whc add a logger
+            self.logger=logging.getLogger()
+            self.logger.setLevel(logging.INFO)
+            fh=logging.FileHandler("test_log/paddle_res.log",mode="w")
+            fh.setLevel(logging.INFO)
+            self.logger.addHandler(fh)
+            #self.checkpoint = CheckpointManager(
+            #    args.experiment_dir,
+            #    keep_interval=cfg.get_int('checkpoint_interval')
+            #)
+        else:
+            self.checkpoint = None
+
+        #fleet.init(is_collective=True)
         self.model = model.MoCoWrapper(cfg).build_moco_diffloss()
 
         self.data_loader_factory = DataLoaderFactoryV3(cfg)
-        self.train_loader = self.data_loader_factory.build(vid=True)
+        self.train_loader = self.data_loader_factory.build(vid=True,device=None)
 
         self.learning_rate = cfg.get_float('optimizer.lr')
         self.loss_lambda = cfg.get_config('loss_lambda')
@@ -37,10 +62,17 @@ class Engine:
             A=self.loss_lambda.get_float('A'),
             M=self.loss_lambda.get_float('M')
         )
-
+        print("####################world_size=",self.args.world_size)
+        if not self.args.no_scale_lr:
+            self.learning_rate = utils.environment.scale_learning_rate(
+                self.learning_rate,
+                self.args.world_size if self.args.world_size==8 else 8,
+                self.batch_size,
+            ) 
+        self.scheduler = paddle.optimizer.lr.CosineAnnealingDecay(learning_rate=self.learning_rate,T_max=self.epochs,eta_min=self.learning_rate/1000)
         self.optimizer = paddle.optimizer.Momentum(
             parameters=self.model.parameters(),
-            learning_rate=self.learning_rate,
+            learning_rate=self.scheduler,
             momentum=cfg.get_float('optimizer.momentum'),
             # dampening=cfg.get_float('optimizer.dampening'),
             weight_decay=cfg.get_float('optimizer.weight_decay'),
@@ -57,7 +89,11 @@ class Engine:
         clip_q = paddle.randn([self.batch_size, 3, 16, 112, 112])
         clip_k = paddle.randn([self.batch_size, 3, 16, 112, 112])
         # for i in range(loop):
-        for i, (clip_q, clip_k) in enumerate(self.train_loader):
+        avg_loss=0
+        nn=0
+        for i, (clip_list, label_tensor, others)  in enumerate(self.train_loader):
+            clip_q, clip_k = clip_list
+            clip_q, clip_k=paddle.to_tensor(clip_q,place=self.device),paddle.to_tensor(clip_k,place=self.device)
             output, target, ranking_logits, ranking_target = self.model(clip_q, clip_k)
             loss, loss_A, loss_M = self.criterion(output, target, ranking_logits, ranking_target)
 
@@ -76,10 +112,29 @@ class Engine:
             # acc1_A_n = accuracy(output[1], target, k=1)
             # acc5_A_n = accuracy(output[1], target, k=5)
 
+            #whc get the avg_loss
+            avg_loss += loss.detach().numpy()[0]
+            nn += 1
+
             # if i > 0 and i % self.log_interval == 0:
             if True:
                 print("loss: %f, loss_A: %f, loss_M: %f" % (loss, loss_A, loss_M))
                 print("acc1_A: %f, acc5_A: %f, acc1_M: %f" % (acc1_A, acc5_A, acc1_M))
+                if fleet.worker_index()==0:
+                    self.logger.info("whc {} {} {} {} {} {} {} {} {}".format(self.current_epoch,i,loss.detach().numpy()[0], loss_A.detach().numpy()[0], loss_M.detach().numpy()[0],acc1_A.detach().numpy()[0], acc5_A.detach().numpy()[0], acc1_M.detach().numpy()[0],datetime.datetime.now()))
+
+        if fleet.worker_index()== 0:
+            avg_loss = avg_loss / nn
+            is_best = avg_loss < self.best_loss
+            self.best_loss = min(avg_loss,self.best_loss)
+            if is_best:
+                best_param=os.path.join(str(self.args.experiment_dir),"best_mode.pdparams")
+                best_optim=os.path.join(str(self.args.experiment_dir),"best_optim.pdparams")
+                best_sche=os.path.join(str(self.args.experiment_dir),"best_sche.pdparams")
+                paddle.save(self.model.state_dict(),best_param)
+                paddle.save(self.optimizer.state_dict(),best_optim)
+                paddle.save(self.scheduler.state_dict(),best_sche) 
+ 
                 # Do logging as late as possible. this will force CUDA sync.
                 # Log numbers from last iteration, just before update
                 # logger.info(
@@ -89,12 +144,14 @@ class Engine:
                 #     f'{self.top1_meter_A_n}\t{self.top5_meter_A_n}'
                 # )
 
-
             
     def run(self):
         while self.current_epoch < self.epochs:
             print("train epoch: %d" % (self.current_epoch))
             self.train_epoch()
+            if fleet.worker_index()==0:
+                self.logger.info("epoch {} {}".format(self.current_epoch,self.scheduler.get_lr()))
+            self.scheduler.step()
             self.current_epoch += 1
 
 

Local Rank: 0
I0310 11:03:58.781543  4223 nccl_context.cc:189] init nccl context nranks: 2 local rank: 1 gpu id: 1 ring id: 0
W0310 11:03:59.368724  4223 device_context.cc:362] Please NOTE: device: 1, GPU Compute Capability: 7.0, Driver API Version: 11.0, Runtime API Version: 10.0
W0310 11:03:59.372980  4223 device_context.cc:372] device: 1, cuDNN Version: 7.6.
========================> 1
backbone:  s3dg
Traceback (most recent call last):
  File "train_paddle.py", line 184, in <module>
    main()
  File "train_paddle.py", line 171, in main
    engine = Engine(args, cfg, local_rank=local_rank)
  File "train_paddle.py", line 53, in __init__
    self.model = model.MoCoWrapper(cfg).build_moco_diffloss()
  File "/home/users/wuhuachao/project/cv_unsupervised/now_code/upload/RSPNet_paddle/model_paddle.py", line 22, in build_moco_diffloss
    base_model_class = get_model_class(backbone)
  File "/home/users/wuhuachao/project/cv_unsupervised/now_code/upload/RSPNet_paddle/models_paddle/__init__.py", line 23, in get_model_class
    raise ValueError('Unknown model architecture "{%s}"' % (arch))
ValueError: Unknown model architecture "{s3dg}"
grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
/usr/local/python378-gcc540/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:26: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  def convert_to_list(value, n, name, dtype=np.int):
argument type:  <class 'str'>
Working tree is dirty. Patch:
diff --git a/config/finetune/ucf101_resnet18.jsonnet b/config/finetune/ucf101_resnet18.jsonnet
index 09f5f8a..61e2e4d 100644
--- a/config/finetune/ucf101_resnet18.jsonnet
+++ b/config/finetune/ucf101_resnet18.jsonnet
@@ -19,5 +19,5 @@ default {
         batch_size: 16 * batch_size_factor,
     },
     optimizer+: {lr: 0.1},
-    num_epochs: 30,
+    num_epochs: 2,
 }
diff --git a/config/optimizer/loss_lambda.libsonnet b/config/optimizer/loss_lambda.libsonnet
index d5da363..5f79fd6 100644
--- a/config/optimizer/loss_lambda.libsonnet
+++ b/config/optimizer/loss_lambda.libsonnet
@@ -1,5 +1,5 @@
 {
     A: 1.0,
-    M: 1.0,
+    M: 0.2,
     F: 1.0,
-}
\ No newline at end of file
+}
diff --git a/config/pretrain/moco-train-base.jsonnet b/config/pretrain/moco-train-base.jsonnet
index 252faac..fb9301f 100644
--- a/config/pretrain/moco-train-base.jsonnet
+++ b/config/pretrain/moco-train-base.jsonnet
@@ -10,7 +10,7 @@ local loss_lambda = import "../optimizer/loss_lambda.libsonnet";
         arch: $.arch,
     },
 
-    dataset: kinetics100, // or kinetics100
+    dataset: kinetics100, // or kinetics400
 
     batch_size: 64,
     num_workers: 4,
diff --git a/config/pretrain/s3dg.jsonnet b/config/pretrain/s3dg.jsonnet
index bf6e94a..f93c786 100644
--- a/config/pretrain/s3dg.jsonnet
+++ b/config/pretrain/s3dg.jsonnet
@@ -1,7 +1,7 @@
 local base = import "moco-train-base.jsonnet";
 
 base {
-    batch_size: 64,
+    batch_size: 32,
     num_workers: 4,
 
     arch: 's3dg',
diff --git a/datasets/classification/__init__.py b/datasets/classification/__init__.py
index be5201b..3485633 100644
--- a/datasets/classification/__init__.py
+++ b/datasets/classification/__init__.py
@@ -140,10 +140,11 @@ class DataLoaderFactoryV3:
         dl = DataLoader(
             video_dataset,
             #batch_size=batch_size,
-            #num_workers=self.cfg.get_int('num_workers'),
-            num_workers=0,
+            num_workers=self.cfg.get_int('num_workers'),
+            #num_workers=0,
             batch_sampler=sampler,
-            # drop_last=(split == 'train'),
+            use_shared_memory =False,
+            #drop_last=(split == 'train'),
             #collate_fn=identity,  # We will deal with collation on main thread.
             collate_fn=gpu_collate_fn,  # We will deal with collation on main thread.
             # multiprocessing_context=mp.get_context('fork'),
diff --git a/datasets/classification/video.py b/datasets/classification/video.py
index fdd79bf..6d22ca3 100644
--- a/datasets/classification/video.py
+++ b/datasets/classification/video.py
@@ -73,11 +73,11 @@ class VideoDataset(Dataset):
         clip_frame_indices_list = [self.temporal_transform(frame_indices) for _ in range(self.num_clips_per_sample)]
         # Fetch all frames in one `vr.get_batch` call
         clip_frame_indices = np.concatenate(clip_frame_indices_list)  # [a1, a2, ..., an, b1, b2, ...,bn]
+        decord.bridge.set_bridge('torch')
         clips: torch.Tensor = vr.get_batch(clip_frame_indices)  # [N*T, H, W, C]
         clip_list = clips.chunk(len(clip_frame_indices_list), dim=0)  # List[Tensor[T, H, W, C]]
-
+        
         clip_list = [self.spatial_transform(clip) for clip in clip_list]
-
         return clip_list, sample.class_index
 
     def __len__(self):
diff --git a/datasets/transforms_video/transforms_tensor.py b/datasets/transforms_video/transforms_tensor.py
index d008074..4f63db6 100644
--- a/datasets/transforms_video/transforms_tensor.py
+++ b/datasets/transforms_video/transforms_tensor.py
@@ -217,7 +217,7 @@ class SequentialGPUCollateFn:
 
         if self.target_transform:
             label_tensor = torch.as_tensor(label)
-            label_tensor = label_tensor.cuda(device=self.device, non_blocking=True)
+            #label_tensor = label_tensor.cuda(device=self.device, non_blocking=True)
         else:
             label_tensor = None
 
@@ -226,10 +226,9 @@ class SequentialGPUCollateFn:
         clip_list: List[List[Optional[Tensor]]] = [[None for _ in range(batch_size)] for _ in range(num_clips)]
         for batch_index, clip in enumerate(clips):  # batch of clip0, clip1
             for clip_index, clip_tensor in enumerate(clip):
-                clip_tensor = clip_tensor.cuda(device=self.device, non_blocking=True)
+                #clip_tensor = clip_tensor.cuda(device=self.device, non_blocking=True)
                 clip_tensor = self.transform(clip_tensor)
                 clip_list[clip_index][batch_index] = clip_tensor
-
         clip_list = [np.array(torch.stack(x, dim=0).cpu()) for x in clip_list]
-        # return (clip_list, label_tensor, *others)
-        return clip_list
+        return (clip_list, label_tensor.cpu().numpy() if label_tensor is not None else np.zeros_like(clip_list[0]), others)
+        #return clip_list
diff --git a/finetune.py b/finetune.py
index 2a47321..a85ea75 100644
--- a/finetune.py
+++ b/finetune.py
@@ -2,11 +2,12 @@ import time
 import logging
 import warnings
 
-import torch
-import torch.distributed as dist
-import torch.multiprocessing as mp
+import paddle
+import paddle.distributed as dist
 from pyhocon import ConfigTree
-from torch import nn
+from paddle import nn
+from paddle.distributed import fleet
+
 from torch.utils.tensorboard import SummaryWriter
 
 from arguments import Args
@@ -423,7 +424,9 @@ class Engine:
             self.summary_writer.flush()
 
 
-def main_worker(local_rank: int, args: Args, dist_url: str):
+def main_worker(args: Args):
+    
+    
     print('Local Rank:', local_rank)
 
     # log in main process only
@@ -485,18 +488,8 @@ def main():
     pack_code(args.run_dir)
 
     utils.environment.ulimit_n_max()
-
-    free_port = utils.distributed.find_free_port()
-    dist_url = f'tcp://127.0.0.1:{free_port}'
-
-    print(f'world_size={args.world_size} Using dist_url={dist_url}')
-
-    """
-    We only consider single node here. 'world_size' is the number of processes.
-    """
-    args.parser = None
-    mp.spawn(main_worker, args=(args, dist_url,), nprocs=args.world_size)
-
+    main_worker(args)
+    
 
 if __name__ == '__main__':
     main()
diff --git a/framework/arguments.py b/framework/arguments.py
index 68f2955..883187a 100644
--- a/framework/arguments.py
+++ b/framework/arguments.py
@@ -77,8 +77,13 @@ class Args(TypedArgs):
         if self.experiment_dir is not None:
             self.experiment_dir.mkdir(parents=True, exist_ok=True)
             if not self.ask_for_replacing_older_dir(self.run_dir):
-                raise EnvironmentError(f'Run dir "{self.run_dir}" exists')
-            self.run_dir.mkdir(parents=True, exist_ok=False)
+                print(f'Run dir "{self.run_dir}" exists')
+                #raise EnvironmentError(f'Run dir "{self.run_dir}" exists')
+            else:
+                try:
+                    self.run_dir.mkdir(parents=True, exist_ok=False)
+                except Exception as e:
+                    print(f'Run dir "{self.run_dir}" exists#################')
 
     def make_experiment_dir(self):
         if not self.ask_for_replacing_older_dir(self.experiment_dir):
@@ -92,9 +97,10 @@ class Args(TypedArgs):
         print(
             f'File exists: {dir_to_be_replaced}\nDo you want to remove it and create a new one?'
         )
-        choice = input('Remove older directory? [y]es/[n]o: ')
-
-        if choice in ['y', 'yes']:
-            shutil.rmtree(dir_to_be_replaced)
-            return True
-        return False
+        #choice = input('Remove older directory? [y]es/[n]o: ')
+        shutil.rmtree(dir_to_be_replaced)
+        return True
+        #if choice in ['y', 'yes']:
+        #    shutil.rmtree(dir_to_be_replaced)
+        #    return True
+        #return False
diff --git a/moco/split_wrapper.py b/moco/split_wrapper.py
index cdb4660..1a981d4 100644
--- a/moco/split_wrapper.py
+++ b/moco/split_wrapper.py
@@ -2,34 +2,33 @@ import functools
 import logging
 from typing import *
 
-import torch
-import torch.nn.functional as F
-from torch import Tensor, nn
+import paddle
+import paddle.nn.functional as F
+from paddle import Tensor, nn
 
 logger = logging.getLogger(__name__)
 
 
-class Flatten(nn.Module):
-
-    def forward(self, x: Tensor):
+class Flatten(nn.Layer):
+    def forward(self, x):
         return x.flatten(1)
 
 
-class ConvFc(nn.Module):
+class ConvFc(nn.Layer):
     """
     conv->relu->conv->downsample->linear
 
     """
 
-    def __init__(self, feat_dim: int, moco_dim: int, kernel_size: Tuple[int, int, int], padding: Tuple[int, int, int]):
+    def __init__(self, feat_dim, moco_dim, kernel_size, padding):
         super().__init__()
-        self.conv1 = nn.Conv3d(feat_dim, feat_dim, kernel_size, padding=padding)
+        self.conv1 = nn.Conv3D(feat_dim, feat_dim, kernel_size, padding=padding)
         self.relu = nn.ReLU(inplace=True)
-        self.conv2 = nn.Conv3d(feat_dim, feat_dim, kernel_size, padding=padding)
-        self.avg_pool = nn.AdaptiveAvgPool3d((1, 1, 1))
+        self.conv2 = nn.Conv3D(feat_dim, feat_dim, kernel_size, padding=padding)
+        self.avg_pool = nn.AdaptiveAvgPool3D((1, 1, 1))
         self.linear = nn.Linear(feat_dim, moco_dim)
 
-    def forward(self, x: Tensor):
+    def forward(self, x):
         out = self.conv1(x)
         out = self.relu(out)
         out = self.conv2(out)
@@ -39,21 +38,21 @@ class ConvFc(nn.Module):
         return out
 
 
-class ConvBnFc(nn.Module):
+class ConvBnFc(nn.Layer):
     """
     conv->relu->conv->downsample->linear
 
     """
 
-    def __init__(self, feat_dim: int, moco_dim: int, kernel_size: Tuple[int, int, int], padding: Tuple[int, int, int]):
+    def __init__(self, feat_dim, moco_dim, kernel_size, padding):
         super().__init__()
-        self.conv1 = nn.Conv3d(feat_dim, feat_dim, kernel_size, padding=padding)
-        self.bn = nn.BatchNorm3d(feat_dim)
+        self.conv1 = nn.Conv3D(feat_dim, feat_dim, kernel_size, padding=padding)
+        self.bn = nn.BatchNorm3D(feat_dim)
         self.relu = nn.ReLU(inplace=True)
-        self.avg_pool = nn.AdaptiveAvgPool3d((1, 1, 1))
+        self.avg_pool = nn.AdaptiveAvgPool3D((1, 1, 1))
         self.linear = nn.Linear(feat_dim, moco_dim)
 
-    def forward(self, x: Tensor):
+    def forward(self, x):
         out = self.conv1(x)
         out = self.bn(out)
         out = self.relu(out)
@@ -63,28 +62,27 @@ class ConvBnFc(nn.Module):
         return out
 
 
-class MultiTaskWrapper(nn.Module):
+class MultiTaskWrapper(nn.Layer):
     """
     This wrapper adds two independent projection layers (one for each pretext task) behind the backbone network.
     The projection layer type can be linear layer and mlp (as indicated in SimCLR).
     """
     def __init__(
             self,
-            base_encoder: Callable[[int], nn.Module],
-            num_classes: int = 128,
-            finetune: bool = False,
-            fc_type: str = 'linear',
-            groups: int = 1,
+            base_encoder,
+            num_classes = 128,
+            finetune = False,
+            fc_type = 'linear',
+            groups = 1,
     ):
         """
-
         :param base_encoder:
         :param num_classes:
         :param finetune:
         :param fc_type:
         :param groups:
         """
-        super().__init__()
+        super(MultiTaskWrapper, self).__init__()
 
         logger.info('Using MultiTask Wrapper')
         self.finetune = finetune
@@ -93,7 +91,7 @@ class MultiTaskWrapper(nn.Module):
         self.groups = groups
         self.fc_type = fc_type
 
-        logger.warning(f'{self.__class__} using groups: {groups}')
+        # logger.warning(f'{self.__class__} using groups: {groups}')
 
         self.encoder = base_encoder(num_classes=1)
 
@@ -101,7 +99,7 @@ class MultiTaskWrapper(nn.Module):
         feat_dim //= groups
 
         if self.finetune:
-            self.avg_pool = nn.AdaptiveAvgPool3d((1, 1, 1))
+            self.avg_pool = nn.AdaptiveAvgPool3D((1, 1, 1))
             self.fc = nn.Linear(feat_dim, num_classes)
         else:
             if fc_type == 'linear':
@@ -124,12 +122,16 @@ class MultiTaskWrapper(nn.Module):
                 self.fc1 = self._get_linear_fc(feat_dim, self.moco_dim)
                 self.fc2 = self._get_linear_fc(feat_dim, 1)  # use for speed binary classification like speednet
 
-    def forward(self, x: Tensor):
-        feat: Tensor = self.encoder.get_feature(x)
-
+    def forward(self, x):
+        print("==========enter==========")
+        feat = self.encoder.get_feature(x)
+        print("==============forward========",feat.shape)
+        
         if self.finetune:
             x3 = self.avg_pool(feat)
+            print("=============MMMMM",x3.shape)
             x3 = x3.flatten(1)
+            print("=========VVVVV===========")
             x3 = self.fc(x3)
             return x3
         else:
@@ -142,26 +144,26 @@ class MultiTaskWrapper(nn.Module):
                 x2 = self.fc2(feat2)
             else:
                 raise Exception
-            x1 = F.normalize(x1, dim=1)
+            x1 = F.normalize(x1, axis=1)
 
             if self.fc_type == 'speednet':  # for speednet, it use sigmoid to ouput the probability that whether the clip is sped up 
-                x2 = torch.sigmoid(x2)
+                x2 = paddle.sigmoid(x2)
             else:
-                x2 = F.normalize(x2, dim=1)
+                x2 = F.normalize(x2, axis=1)
             return x1, x2
 
     @staticmethod
-    def _get_linear_fc(feat_dim: int, moco_dim: int):
+    def _get_linear_fc(feat_dim, moco_dim):
         return nn.Sequential(
-            nn.AdaptiveAvgPool3d((1, 1, 1)),
+            nn.AdaptiveAvgPool3D((1, 1, 1)),
             Flatten(),
             nn.Linear(feat_dim, moco_dim),
         )
 
     @staticmethod
-    def _get_mlp_fc(feat_dim: int, moco_dim: int):
+    def _get_mlp_fc(feat_dim, moco_dim):
         return nn.Sequential(
-            nn.AdaptiveAvgPool3d((1, 1, 1)),
+            nn.AdaptiveAvgPool3D((1, 1, 1)),
             Flatten(),
             nn.Linear(feat_dim, feat_dim),
             nn.ReLU(inplace=True),
@@ -174,7 +176,7 @@ class MultiTaskWrapper(nn.Module):
         feat_dim = 512
         for fc_name in fc_names:
             if hasattr(encoder, fc_name):
-                feat_dim = getattr(encoder, fc_name).in_features
-                logger.info(f'Found fc: {fc_name} with in_features: {feat_dim}')
+                feat_dim = getattr(encoder, fc_name).weight.shape[0]
+                logger.warning('Found fc: %s with in_features: %d' % (fc_name, feat_dim))
                 break
         return feat_dim
diff --git a/model_paddle.py b/model_paddle.py
index cf7c6a9..338b70f 100644
--- a/model_paddle.py
+++ b/model_paddle.py
@@ -189,15 +189,15 @@ class MoCoDiffLossTwoFc(nn.Layer):
     @paddle.no_grad()
     def _forward_encoder_k(self, im_k):
         # shuffle for making use of BN
-        im_k, idx_unshuffle = self._batch_shuffle_ddp(im_k)
+        #im_k, idx_unshuffle = self._batch_shuffle_ddp(im_k)
 
         k_A, k_M = self.encoder_k(im_k)  # keys: NxC
         # k_A = self.encoder_q(im_k)
         # k_M = self.encoder_q(im_k)
 
         # undo shuffle
-        k_A = self._batch_unshuffle_ddp(k_A, idx_unshuffle)
-        k_M = self._batch_unshuffle_ddp(k_M, idx_unshuffle)
+        #k_A = self._batch_unshuffle_ddp(k_A, idx_unshuffle)
+        #k_M = self._batch_unshuffle_ddp(k_M, idx_unshuffle)
 
         return k_A, k_M
 
@@ -305,6 +305,7 @@ def concat_all_gather(tensor):
 
     tensors_gather = [paddle.ones_like(tensor)
         for _ in range(paddle.distributed.get_world_size())]
+    tensors_gather=[]
     paddle.distributed.all_gather(tensors_gather, tensor)
 
     output = paddle.concat(tensors_gather, axis=0)
diff --git a/models/__init__.py b/models/__init__.py
index 0474b23..02c0bef 100644
--- a/models/__init__.py
+++ b/models/__init__.py
@@ -4,16 +4,19 @@ Abstract the process of creating a model into get_model_class, model_class(num_c
 """
 
 from pyhocon import ConfigTree, ConfigFactory
-from torch import nn
-import torch
-from torch import nn
+#from torch import nn
+#import torch
+#from torch import nn
 from typing import *
 import logging
 
+import paddle
+from paddle import nn
+
 logger = logging.getLogger(__name__)
 
 
-def get_model_class(**kwargs) -> Callable[[int], nn.Module]:
+def get_model_class(**kwargs) -> Callable[[int], nn.Layer]:
     """
     Pass the model config as parameters. For convinence, we change the cfg to dict, and then reverse it
     :param kwargs:
@@ -80,8 +83,9 @@ class ModelFactory:
     def __init__(self, cfg: ConfigTree):
         self.cfg = cfg
 
-    def _post_process_model(self, model: nn.Module):
+    def _post_process_model(self, model: nn.Layer):
         if self.cfg.get_bool('only_train_fc', False):
+            print("===============fix some layer===============>")
             for param in model.parameters():
                 param.requires_grad = False
 
@@ -105,7 +109,7 @@ class ModelFactory:
 
         return model
 
-    def build(self, local_rank: int) -> nn.Module:
+    def build(self, local_rank: int) -> nn.Layer:
         # arch = self.cfg.get_string('model.arch')
         num_classes = self.cfg.get_int('dataset.num_classes')
 
@@ -114,30 +118,30 @@ class ModelFactory:
         model = model_class(num_classes=num_classes)
         model = self._post_process_model(model)
 
-        model = model.cuda(local_rank)
-
-        model = nn.parallel.DistributedDataParallel(
-            model,
-            device_ids=[local_rank],
-        )
+        #model = model.cuda(local_rank)
+        #model = nn.parallel.DistributedDataParallel(
+        #    model,
+        #    device_ids=[local_rank],
+        #)
         return model
 
-    def build_multitask_wrapper(self, local_rank: int) -> nn.Module:
+    def build_multitask_wrapper(self, local_rank: int) -> nn.Layer:
         # arch = self.cfg.get_string('model.arch')
 
         from moco.split_wrapper import MultiTaskWrapper
         num_classes = self.cfg.get_int('dataset.num_classes')
 
         model_class = get_model_class(**self.cfg.get_config('model'))
-
+       
+        print("============model_class============>",model_class)
+        
         model = MultiTaskWrapper(model_class, num_classes=num_classes, finetune=True)
         model = self._post_process_model(model)
 
-        model = model.cuda(local_rank)
-
-        model = nn.parallel.DistributedDataParallel(
-            model,
-            device_ids=[local_rank],
-            find_unused_parameters=True,  # some of forward output are not involved in calculation
-        )
+        #model = model.cuda(local_rank)
+        #model = nn.parallel.DistributedDataParallel(
+        #    model,
+        #    device_ids=[local_rank],
+        #    find_unused_parameters=True,  # some of forward output are not involved in calculation
+        #)
         return model
diff --git a/models/resnet.py b/models/resnet.py
index 6f0b294..347059a 100644
--- a/models/resnet.py
+++ b/models/resnet.py
@@ -6,9 +6,10 @@ Commit id: 4e2195c
 import math
 from functools import partial
 
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
+import paddle
+import paddle.nn as nn
+import paddle.nn.functional as F
+import numpy as np
 
 __all__ = [
     'ResNet', 'resnet10', 'resnet18', 'resnet34', 'resnet50', 'resnet101',
@@ -18,43 +19,43 @@ __all__ = [
 
 def conv3x3x3(in_planes, out_planes, stride=1):
     # 3x3x3 convolution with padding
-    return nn.Conv3d(
+    return nn.Conv3D(
         in_planes,
         out_planes,
         kernel_size=3,
         stride=stride,
         padding=1,
-        bias=False)
+        bias_attr=False)
 
 
 def downsample_basic_block(x, planes, stride):
-    out = F.avg_pool3d(x, kernel_size=1, stride=stride)
-    # zero_pads = torch.Tensor(
+    out = F.avg_pool3D(x, kernel_size=1, stride=stride)
+    # zero_pads = paddle.Tensor(
     #     out.size(0), planes - out.size(1), out.size(2), out.size(3),
     #     out.size(4)).zero_()
-    # if isinstance(out.data, torch.cuda.FloatTensor):
+    # if isinstance(out.data, paddle.cuda.FloatTensor):
     #     zero_pads = zero_pads.cuda()
-    zero_pads = torch.zeros(
+    zero_pads = paddle.zeros(
         out.shape[0], planes - out.shape[1], out.shape[2], out.shape[3],
         device=x.device
     )
 
-    # out = Variable(torch.cat([out.data, zero_pads], dim=1))
-    out = torch.cat([out.data, zero_pads], dim=1)
+    # out = Variable(paddle.cat([out.data, zero_pads], dim=1))
+    out = paddle.cat([out.data, zero_pads], dim=1)
 
     return out
 
 
-class BasicBlock(nn.Module):
+class BasicBlock(nn.Layer):
     expansion = 1
 
     def __init__(self, inplanes, planes, stride=1, downsample=None):
         super(BasicBlock, self).__init__()
         self.conv1 = conv3x3x3(inplanes, planes, stride)
-        self.bn1 = nn.BatchNorm3d(planes)
-        self.relu = nn.ReLU(inplace=True)
+        self.bn1 = nn.BatchNorm3D(planes)
+        self.relu = nn.ReLU()
         self.conv2 = conv3x3x3(planes, planes)
-        self.bn2 = nn.BatchNorm3d(planes)
+        self.bn2 = nn.BatchNorm3D(planes)
         self.downsample = downsample
         self.stride = stride
 
@@ -77,19 +78,19 @@ class BasicBlock(nn.Module):
         return out
 
 
-class Bottleneck(nn.Module):
+class Bottleneck(nn.Layer):
     expansion = 4
 
     def __init__(self, inplanes, planes, stride=1, downsample=None):
         super(Bottleneck, self).__init__()
-        self.conv1 = nn.Conv3d(inplanes, planes, kernel_size=1, bias=False)
-        self.bn1 = nn.BatchNorm3d(planes)
-        self.conv2 = nn.Conv3d(
-            planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
-        self.bn2 = nn.BatchNorm3d(planes)
-        self.conv3 = nn.Conv3d(planes, planes * 4, kernel_size=1, bias=False)
-        self.bn3 = nn.BatchNorm3d(planes * 4)
-        self.relu = nn.ReLU(inplace=True)
+        self.conv1 = nn.Conv3D(inplanes, planes, kernel_size=1, bias_attr=False)
+        self.bn1 = nn.BatchNorm3D(planes)
+        self.conv2 = nn.Conv3D(
+            planes, planes, kernel_size=3, stride=stride, padding=1, bias_attr=False)
+        self.bn2 = nn.BatchNorm3D(planes)
+        self.conv3 = nn.Conv3D(planes, planes * 4, kernel_size=1, bias_attr=False)
+        self.bn3 = nn.BatchNorm3D(planes * 4)
+        self.relu = nn.ReLU()
         self.downsample = downsample
         self.stride = stride
 
@@ -116,7 +117,7 @@ class Bottleneck(nn.Module):
         return out
 
 
-class ResNet(nn.Module):
+class ResNet(nn.Layer):
 
     def __init__(self,
                  block,
@@ -127,16 +128,16 @@ class ResNet(nn.Module):
                  num_classes=400):
         self.inplanes = 64
         super(ResNet, self).__init__()
-        self.conv1 = nn.Conv3d(
+        self.conv1 = nn.Conv3D(
             3,
             64,
             kernel_size=7,
             stride=(1, 2, 2),
             padding=(3, 3, 3),
-            bias=False)
-        self.bn1 = nn.BatchNorm3d(64)
-        self.relu = nn.ReLU(inplace=True)
-        self.maxpool = nn.MaxPool3d(kernel_size=(3, 3, 3), stride=2, padding=1)
+            bias_attr=False)
+        self.bn1 = nn.BatchNorm3D(64)
+        self.relu = nn.ReLU()
+        self.maxpool = nn.MaxPool3D(kernel_size=(3, 3, 3), stride=2, padding=1)
         self.layer1 = self._make_layer(block, 64, layers[0], shortcut_type)
         self.layer2 = self._make_layer(
             block, 128, layers[1], shortcut_type, stride=2)
@@ -146,16 +147,24 @@ class ResNet(nn.Module):
             block, 512, layers[3], shortcut_type, stride=2)
         last_duration = int(math.ceil(sample_duration / 16))
         last_size = int(math.ceil(sample_size / 32))
-        self.avgpool = nn.AvgPool3d(
+        self.avgpool = nn.AvgPool3D(
             (last_duration, last_size, last_size), stride=1)
         self.fc = nn.Linear(512 * block.expansion, num_classes)
-
-        for m in self.modules():
-            if isinstance(m, nn.Conv3d):
-                m.weight = nn.init.kaiming_normal_(m.weight, mode='fan_out')
-            elif isinstance(m, nn.BatchNorm3d):
-                m.weight.data.fill_(1)
-                m.bias.data.zero_()
+        # self.fc = nn.Linear(512 * 4, num_classes)
+
+        # for m in self.modules():
+        # for m in self.sublayers():
+        #     if isinstance(m, nn.Conv3D):
+        #         pass
+        #         # m.weight = nn.init.kaiming_normal_(m.weight, mode='fan_out')
+        #         # print("m.weight", m.weight)
+        #     elif isinstance(m, nn.BatchNorm3D):
+        #         print("type(m.weight)", type(m.weight))
+        #         print("before m.weight", m.weight)
+        #         # m.weight.data.fill_(1)
+        #         # m.bias.data.zero_()
+        #         m.weight = paddle.zeros_like(m.weight)
+        #         print("after m.weight", m.weight)
 
     def _make_layer(self, block, planes, blocks, shortcut_type, stride=1):
         downsample = None
@@ -167,12 +176,12 @@ class ResNet(nn.Module):
                     stride=stride)
             else:
                 downsample = nn.Sequential(
-                    nn.Conv3d(
+                    nn.Conv3D(
                         self.inplanes,
                         planes * block.expansion,
                         kernel_size=1,
                         stride=stride,
-                        bias=False), nn.BatchNorm3d(planes * block.expansion))
+                        bias_attr=False), nn.BatchNorm3D(planes * block.expansion))
 
         layers = []
         layers.append(block(self.inplanes, planes, stride, downsample))
@@ -187,17 +196,16 @@ class ResNet(nn.Module):
         x = self.bn1(x)
         x = self.relu(x)
         x = self.maxpool(x)
-
+        
         x = self.layer1(x)
         x = self.layer2(x)
         x = self.layer3(x)
         x = self.layer4(x)
 
         x = self.avgpool(x)
-
-        x = x.view(x.size(0), -1)
+        x = paddle.reshape(x, shape=[paddle.shape(x)[0], -1])
         x = self.fc(x)
-
+        
         return x
 
     def get_feature(self, x):
@@ -217,7 +225,7 @@ class ResNet(nn.Module):
 
         x = self.avgpool(feat)
 
-        x = x.view(x.size(0), -1)
+        x = paddle.reshape(x, shape=[paddle.shape(x)[0], -1])
         x = self.fc(x)
 
         return x, feat
@@ -294,8 +302,8 @@ def resnet200(**kwargs):
 
 
 if __name__ == '__main__':
-    x = torch.rand(2, 3, 16, 112, 112)
+    x = paddle.randn([2, 3, 16, 112, 112])
     m = resnet18()
-    print(m)
+    # print(m)
     y = m(x)
-    print(y.shape)
+    # print(y.shape)
diff --git a/models_paddle/__init__.py b/models_paddle/__init__.py
index f9ac301..fa0187a 100644
--- a/models_paddle/__init__.py
+++ b/models_paddle/__init__.py
@@ -19,6 +19,9 @@ def get_model_class(arch):
     elif arch == 'resnet50':
         from .resnet import resnet50
         model_class = resnet50
+    elif arch == 's3dg':
+        from .s3dg import S3D_G
+        model_class = S3D_G
     else:
         raise ValueError('Unknown model architecture "{%s}"' % (arch))
 
diff --git a/pretrain.py b/pretrain.py
index f9feda9..0696980 100644
--- a/pretrain.py
+++ b/pretrain.py
@@ -24,7 +24,7 @@ from framework.utils.checkpoint import CheckpointManager
 from moco import ModelFactory
 from moco.builder_diffspeed_diffloss import Loss
 from utils.moco import replace_moco_k_in_config
-
+import logging
 logger = logging.getLogger(__name__)
 
 
@@ -51,7 +51,14 @@ class Engine:
             A=self.loss_lambda.get_float('A'),
             M=self.loss_lambda.get_float('M')
         )
-
+        
+        #whc add a logger
+        self.logger=logging.getLogger()
+        self.logger.setLevel(logging.INFO)
+        fh=logging.FileHandler("test_log/torch_res.log",mode="w")
+        fh.setLevel(logging.INFO)
+        self.logger.addHandler(fh)
+        
         self.train_loader = self.data_loader_factory.build(vid=True, device=self.device)
 
         self.learning_rate = self.cfg.get_float('optimizer.lr')
@@ -173,7 +180,9 @@ class Engine:
             batch_size = len(clip_q)
             self.top1_meter_A_n.update(acc1_A_n, batch_size)
             self.top5_meter_A_n.update(acc5_A_n, batch_size)
-
+            
+            #whc
+            self.logger.info("whc {} {} {} {} {} {} {} {}".format(self.current_epoch,i,loss.detach().numpy()[0], loss_A.detach().numpy()[0], loss_M.detach().numpy()[0],acc1_A.detach().numpy()[0], acc5_A.detach().numpy()[0], acc1_M.detach().numpy()[0]))
             if i > 0 and i % self.log_interval == 0:
                 # Do logging as late as possible. this will force CUDA sync.
                 # Log numbers from last iteration, just before update
diff --git a/train_paddle.py b/train_paddle.py
index 142f2b7..f1bc1a8 100644
--- a/train_paddle.py
+++ b/train_paddle.py
@@ -6,14 +6,18 @@ import paddle.distributed.spawn as spawn
 import numpy as np
 import model_paddle as model
 import models.resnet as resnet
-
+import os
 from arguments import Args
 from framework import utils
-
 from datasets.classification import DataLoaderFactoryV3
 from framework.config import get_config, save_config
+import paddle.distributed.fleet.base.role_maker as role_maker
+#from framework.utils.whc_checkpoint import CheckpointManager
 from paddle.distributed import fleet
-
+#whc
+import logging
+import time
+import datetime
 
 class Engine:
     def __init__(self, args, cfg, local_rank):
@@ -22,13 +26,34 @@ class Engine:
         self.current_epoch = 0
         # self.batch_size = self.args.train_batch_size
         self.batch_size = cfg.get_int('batch_size')
-        self.batch_size = 64
-
-        fleet.init(is_collective=True)
+        #self.batch_size = 64
+        self.local_rank=local_rank
+        self.best_loss = float('inf')
+        self.arch = cfg.get_string('arch')
+        role = role_maker.PaddleCloudRoleMaker(is_collective=True)
+        fleet.init(role)
+       
+        self.device = paddle.CUDAPlace(fleet.worker_index()) 
+        print("========================>",fleet.worker_index()) 
+        if fleet.worker_index()==0:
+            #whc add a logger
+            self.logger=logging.getLogger()
+            self.logger.setLevel(logging.INFO)
+            fh=logging.FileHandler("test_log/paddle_res.log",mode="w")
+            fh.setLevel(logging.INFO)
+            self.logger.addHandler(fh)
+            #self.checkpoint = CheckpointManager(
+            #    args.experiment_dir,
+            #    keep_interval=cfg.get_int('checkpoint_interval')
+            #)
+        else:
+            self.checkpoint = None
+
+        #fleet.init(is_collective=True)
         self.model = model.MoCoWrapper(cfg).build_moco_diffloss()
 
         self.data_loader_factory = DataLoaderFactoryV3(cfg)
-        self.train_loader = self.data_loader_factory.build(vid=True)
+        self.train_loader = self.data_loader_factory.build(vid=True,device=None)
 
         self.learning_rate = cfg.get_float('optimizer.lr')
         self.loss_lambda = cfg.get_config('loss_lambda')
@@ -37,10 +62,17 @@ class Engine:
             A=self.loss_lambda.get_float('A'),
             M=self.loss_lambda.get_float('M')
         )
-
+        print("####################world_size=",self.args.world_size)
+        if not self.args.no_scale_lr:
+            self.learning_rate = utils.environment.scale_learning_rate(
+                self.learning_rate,
+                self.args.world_size if self.args.world_size==8 else 8,
+                self.batch_size,
+            ) 
+        self.scheduler = paddle.optimizer.lr.CosineAnnealingDecay(learning_rate=self.learning_rate,T_max=self.epochs,eta_min=self.learning_rate/1000)
         self.optimizer = paddle.optimizer.Momentum(
             parameters=self.model.parameters(),
-            learning_rate=self.learning_rate,
+            learning_rate=self.scheduler,
             momentum=cfg.get_float('optimizer.momentum'),
             # dampening=cfg.get_float('optimizer.dampening'),
             weight_decay=cfg.get_float('optimizer.weight_decay'),
@@ -57,7 +89,11 @@ class Engine:
         clip_q = paddle.randn([self.batch_size, 3, 16, 112, 112])
         clip_k = paddle.randn([self.batch_size, 3, 16, 112, 112])
         # for i in range(loop):
-        for i, (clip_q, clip_k) in enumerate(self.train_loader):
+        avg_loss=0
+        nn=0
+        for i, (clip_list, label_tensor, others)  in enumerate(self.train_loader):
+            clip_q, clip_k = clip_list
+            clip_q, clip_k=paddle.to_tensor(clip_q,place=self.device),paddle.to_tensor(clip_k,place=self.device)
             output, target, ranking_logits, ranking_target = self.model(clip_q, clip_k)
             loss, loss_A, loss_M = self.criterion(output, target, ranking_logits, ranking_target)
 
@@ -76,10 +112,29 @@ class Engine:
             # acc1_A_n = accuracy(output[1], target, k=1)
             # acc5_A_n = accuracy(output[1], target, k=5)
 
+            #whc get the avg_loss
+            avg_loss += loss.detach().numpy()[0]
+            nn += 1
+
             # if i > 0 and i % self.log_interval == 0:
             if True:
                 print("loss: %f, loss_A: %f, loss_M: %f" % (loss, loss_A, loss_M))
                 print("acc1_A: %f, acc5_A: %f, acc1_M: %f" % (acc1_A, acc5_A, acc1_M))
+                if fleet.worker_index()==0:
+                    self.logger.info("whc {} {} {} {} {} {} {} {} {}".format(self.current_epoch,i,loss.detach().numpy()[0], loss_A.detach().numpy()[0], loss_M.detach().numpy()[0],acc1_A.detach().numpy()[0], acc5_A.detach().numpy()[0], acc1_M.detach().numpy()[0],datetime.datetime.now()))
+
+        if fleet.worker_index()== 0:
+            avg_loss = avg_loss / nn
+            is_best = avg_loss < self.best_loss
+            self.best_loss = min(avg_loss,self.best_loss)
+            if is_best:
+                best_param=os.path.join(str(self.args.experiment_dir),"best_mode.pdparams")
+                best_optim=os.path.join(str(self.args.experiment_dir),"best_optim.pdparams")
+                best_sche=os.path.join(str(self.args.experiment_dir),"best_sche.pdparams")
+                paddle.save(self.model.state_dict(),best_param)
+                paddle.save(self.optimizer.state_dict(),best_optim)
+                paddle.save(self.scheduler.state_dict(),best_sche) 
+ 
                 # Do logging as late as possible. this will force CUDA sync.
                 # Log numbers from last iteration, just before update
                 # logger.info(
@@ -89,12 +144,14 @@ class Engine:
                 #     f'{self.top1_meter_A_n}\t{self.top5_meter_A_n}'
                 # )
 
-
             
     def run(self):
         while self.current_epoch < self.epochs:
             print("train epoch: %d" % (self.current_epoch))
             self.train_epoch()
+            if fleet.worker_index()==0:
+                self.logger.info("epoch {} {}".format(self.current_epoch,self.scheduler.get_lr()))
+            self.scheduler.step()
             self.current_epoch += 1
 
 

Local Rank: 0
I0310 11:07:09.516283  4538 nccl_context.cc:189] init nccl context nranks: 2 local rank: 1 gpu id: 1 ring id: 0
W0310 11:07:10.079443  4538 device_context.cc:362] Please NOTE: device: 1, GPU Compute Capability: 7.0, Driver API Version: 11.0, Runtime API Version: 10.0
W0310 11:07:10.083477  4538 device_context.cc:372] device: 1, cuDNN Version: 7.6.
========================> 1
backbone:  s3dg
Found fc: fc with in_features: 1024
Found fc: fc with in_features: 1024
####################world_size= 2
adjust lr according to the number of GPU and batch size：0.05 -> 0.2
train epoch: 0
/usr/local/python378-gcc540/lib/python3.7/site-packages/paddle/nn/layer/norm.py:636: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
[128, 16384]
loss: 18.567331, loss_A: 18.184803, loss_M: 1.912639
acc1_A: 0.031250, acc5_A: 0.031250, acc1_M: 0.625000
[128, 16384]
loss: 7.927181, loss_A: 7.507280, loss_M: 2.099507
acc1_A: 0.031250, acc5_A: 0.312500, acc1_M: 0.437500
[128, 16384]
Traceback (most recent call last):
  File "train_paddle.py", line 184, in <module>
    main()
  File "train_paddle.py", line 172, in main
    engine.run()
  File "train_paddle.py", line 151, in run
    self.train_epoch()
  File "train_paddle.py", line 103, in train_epoch
    self.model.clear_gradients()
  File "/usr/local/python378-gcc540/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py", line 869, in clear_gradients
    p.clear_gradient()
KeyboardInterrupt
