grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
/home/users/wuhuachao/RSPNet_paddle/framework/config.py:41: DeprecationWarning: invalid escape sequence \d
  arg_regex = re.compile('^__arg_(\d+)__$')
argument type:  <class 'str'>
Run dir "exps/pretext-resnet18/run_0_20210304_132126" exists#################
Working tree is dirty. Patch:
diff --git a/config/pretrain/moco-train-base.jsonnet b/config/pretrain/moco-train-base.jsonnet
index 252faac..fb9301f 100644
--- a/config/pretrain/moco-train-base.jsonnet
+++ b/config/pretrain/moco-train-base.jsonnet
@@ -10,7 +10,7 @@ local loss_lambda = import "../optimizer/loss_lambda.libsonnet";
         arch: $.arch,
     },
 
-    dataset: kinetics100, // or kinetics100
+    dataset: kinetics100, // or kinetics400
 
     batch_size: 64,
     num_workers: 4,
diff --git a/framework/arguments.py b/framework/arguments.py
index 68f2955..883187a 100644
--- a/framework/arguments.py
+++ b/framework/arguments.py
@@ -77,8 +77,13 @@ class Args(TypedArgs):
         if self.experiment_dir is not None:
             self.experiment_dir.mkdir(parents=True, exist_ok=True)
             if not self.ask_for_replacing_older_dir(self.run_dir):
-                raise EnvironmentError(f'Run dir "{self.run_dir}" exists')
-            self.run_dir.mkdir(parents=True, exist_ok=False)
+                print(f'Run dir "{self.run_dir}" exists')
+                #raise EnvironmentError(f'Run dir "{self.run_dir}" exists')
+            else:
+                try:
+                    self.run_dir.mkdir(parents=True, exist_ok=False)
+                except Exception as e:
+                    print(f'Run dir "{self.run_dir}" exists#################')
 
     def make_experiment_dir(self):
         if not self.ask_for_replacing_older_dir(self.experiment_dir):
@@ -92,9 +97,10 @@ class Args(TypedArgs):
         print(
             f'File exists: {dir_to_be_replaced}\nDo you want to remove it and create a new one?'
         )
-        choice = input('Remove older directory? [y]es/[n]o: ')
-
-        if choice in ['y', 'yes']:
-            shutil.rmtree(dir_to_be_replaced)
-            return True
-        return False
+        #choice = input('Remove older directory? [y]es/[n]o: ')
+        shutil.rmtree(dir_to_be_replaced)
+        return True
+        #if choice in ['y', 'yes']:
+        #    shutil.rmtree(dir_to_be_replaced)
+        #    return True
+        #return False
diff --git a/model_paddle.py b/model_paddle.py
index cf7c6a9..338b70f 100644
--- a/model_paddle.py
+++ b/model_paddle.py
@@ -189,15 +189,15 @@ class MoCoDiffLossTwoFc(nn.Layer):
     @paddle.no_grad()
     def _forward_encoder_k(self, im_k):
         # shuffle for making use of BN
-        im_k, idx_unshuffle = self._batch_shuffle_ddp(im_k)
+        #im_k, idx_unshuffle = self._batch_shuffle_ddp(im_k)
 
         k_A, k_M = self.encoder_k(im_k)  # keys: NxC
         # k_A = self.encoder_q(im_k)
         # k_M = self.encoder_q(im_k)
 
         # undo shuffle
-        k_A = self._batch_unshuffle_ddp(k_A, idx_unshuffle)
-        k_M = self._batch_unshuffle_ddp(k_M, idx_unshuffle)
+        #k_A = self._batch_unshuffle_ddp(k_A, idx_unshuffle)
+        #k_M = self._batch_unshuffle_ddp(k_M, idx_unshuffle)
 
         return k_A, k_M
 
@@ -305,6 +305,7 @@ def concat_all_gather(tensor):
 
     tensors_gather = [paddle.ones_like(tensor)
         for _ in range(paddle.distributed.get_world_size())]
+    tensors_gather=[]
     paddle.distributed.all_gather(tensors_gather, tensor)
 
     output = paddle.concat(tensors_gather, axis=0)
diff --git a/models_paddle/__init__.py b/models_paddle/__init__.py
index f9ac301..fa0187a 100644
--- a/models_paddle/__init__.py
+++ b/models_paddle/__init__.py
@@ -19,6 +19,9 @@ def get_model_class(arch):
     elif arch == 'resnet50':
         from .resnet import resnet50
         model_class = resnet50
+    elif arch == 's3dg':
+        from .s3dg import S3D_G
+        model_class = S3D_G
     else:
         raise ValueError('Unknown model architecture "{%s}"' % (arch))
 
diff --git a/pretrain.py b/pretrain.py
index f9feda9..0696980 100644
--- a/pretrain.py
+++ b/pretrain.py
@@ -24,7 +24,7 @@ from framework.utils.checkpoint import CheckpointManager
 from moco import ModelFactory
 from moco.builder_diffspeed_diffloss import Loss
 from utils.moco import replace_moco_k_in_config
-
+import logging
 logger = logging.getLogger(__name__)
 
 
@@ -51,7 +51,14 @@ class Engine:
             A=self.loss_lambda.get_float('A'),
             M=self.loss_lambda.get_float('M')
         )
-
+        
+        #whc add a logger
+        self.logger=logging.getLogger()
+        self.logger.setLevel(logging.INFO)
+        fh=logging.FileHandler("test_log/torch_res.log",mode="w")
+        fh.setLevel(logging.INFO)
+        self.logger.addHandler(fh)
+        
         self.train_loader = self.data_loader_factory.build(vid=True, device=self.device)
 
         self.learning_rate = self.cfg.get_float('optimizer.lr')
@@ -173,7 +180,9 @@ class Engine:
             batch_size = len(clip_q)
             self.top1_meter_A_n.update(acc1_A_n, batch_size)
             self.top5_meter_A_n.update(acc5_A_n, batch_size)
-
+            
+            #whc
+            self.logger.info("whc {} {} {} {} {} {} {} {}".format(self.current_epoch,i,loss.detach().numpy()[0], loss_A.detach().numpy()[0], loss_M.detach().numpy()[0],acc1_A.detach().numpy()[0], acc5_A.detach().numpy()[0], acc1_M.detach().numpy()[0]))
             if i > 0 and i % self.log_interval == 0:
                 # Do logging as late as possible. this will force CUDA sync.
                 # Log numbers from last iteration, just before update
diff --git a/train_paddle.py b/train_paddle.py
index 142f2b7..acaecb3 100644
--- a/train_paddle.py
+++ b/train_paddle.py
@@ -6,14 +6,16 @@ import paddle.distributed.spawn as spawn
 import numpy as np
 import model_paddle as model
 import models.resnet as resnet
-
+import os
 from arguments import Args
 from framework import utils
-
 from datasets.classification import DataLoaderFactoryV3
 from framework.config import get_config, save_config
+import paddle.distributed.fleet.base.role_maker as role_maker
+#from framework.utils.whc_checkpoint import CheckpointManager
 from paddle.distributed import fleet
-
+#whc
+import logging
 
 class Engine:
     def __init__(self, args, cfg, local_rank):
@@ -22,9 +24,28 @@ class Engine:
         self.current_epoch = 0
         # self.batch_size = self.args.train_batch_size
         self.batch_size = cfg.get_int('batch_size')
-        self.batch_size = 64
-
+        #self.batch_size = 64
+        self.local_rank=local_rank
+        self.best_loss = float('inf')
+        self.arch = cfg.get_string('arch')
+        role = role_maker.PaddleCloudRoleMaker(is_collective=True)
         fleet.init(is_collective=True)
+        
+        if fleet.worker_index()==0:
+            #whc add a logger
+            self.logger=logging.getLogger()
+            self.logger.setLevel(logging.INFO)
+            fh=logging.FileHandler("test_log/paddle_res.log",mode="w")
+            fh.setLevel(logging.INFO)
+            self.logger.addHandler(fh)
+            #self.checkpoint = CheckpointManager(
+            #    args.experiment_dir,
+            #    keep_interval=cfg.get_int('checkpoint_interval')
+            #)
+        else:
+            self.checkpoint = None
+
+        #fleet.init(is_collective=True)
         self.model = model.MoCoWrapper(cfg).build_moco_diffloss()
 
         self.data_loader_factory = DataLoaderFactoryV3(cfg)
@@ -37,10 +58,17 @@ class Engine:
             A=self.loss_lambda.get_float('A'),
             M=self.loss_lambda.get_float('M')
         )
-
+        print("####################world_size=",self.args.world_size)
+        if not self.args.no_scale_lr:
+            self.learning_rate = utils.environment.scale_learning_rate(
+                self.learning_rate,
+                self.args.world_size if self.args.world_size==8 else 8,
+                self.batch_size,
+            ) 
+        self.scheduler = paddle.optimizer.lr.CosineAnnealingDecay(learning_rate=self.learning_rate,T_max=self.epochs,eta_min=self.learning_rate/1000)
         self.optimizer = paddle.optimizer.Momentum(
             parameters=self.model.parameters(),
-            learning_rate=self.learning_rate,
+            learning_rate=self.scheduler,
             momentum=cfg.get_float('optimizer.momentum'),
             # dampening=cfg.get_float('optimizer.dampening'),
             weight_decay=cfg.get_float('optimizer.weight_decay'),
@@ -57,6 +85,8 @@ class Engine:
         clip_q = paddle.randn([self.batch_size, 3, 16, 112, 112])
         clip_k = paddle.randn([self.batch_size, 3, 16, 112, 112])
         # for i in range(loop):
+        avg_loss=0
+        nn=0
         for i, (clip_q, clip_k) in enumerate(self.train_loader):
             output, target, ranking_logits, ranking_target = self.model(clip_q, clip_k)
             loss, loss_A, loss_M = self.criterion(output, target, ranking_logits, ranking_target)
@@ -76,10 +106,29 @@ class Engine:
             # acc1_A_n = accuracy(output[1], target, k=1)
             # acc5_A_n = accuracy(output[1], target, k=5)
 
+            #whc get the avg_loss
+            avg_loss += loss.detach().numpy()[0]
+            nn += 1
+
             # if i > 0 and i % self.log_interval == 0:
             if True:
                 print("loss: %f, loss_A: %f, loss_M: %f" % (loss, loss_A, loss_M))
                 print("acc1_A: %f, acc5_A: %f, acc1_M: %f" % (acc1_A, acc5_A, acc1_M))
+                if fleet.worker_index()==0:
+                    self.logger.info("whc {} {} {} {} {} {} {} {}".format(self.current_epoch,i,loss.detach().numpy()[0], loss_A.detach().numpy()[0], loss_M.detach().numpy()[0],acc1_A.detach().numpy()[0], acc5_A.detach().numpy()[0], acc1_M.detach().numpy()[0]))
+
+        if fleet.worker_index()== 0:
+            avg_loss = avg_loss / nn
+            is_best = avg_loss < self.best_loss
+            self.best_loss = min(avg_loss,self.best_loss)
+            if is_best:
+                best_param=os.path.join(str(self.args.experiment_dir),"best_mode.pdparams")
+                best_optim=os.path.join(str(self.args.experiment_dir),"best_optim.pdparams")
+                best_sche=os.path.join(str(self.args.experiment_dir),"best_sche.pdparams")
+                paddle.save(self.model.state_dict(),best_param)
+                paddle.save(self.optimizer.state_dict(),best_optim)
+                paddle.save(self.scheduler.state_dict(),best_sche) 
+ 
                 # Do logging as late as possible. this will force CUDA sync.
                 # Log numbers from last iteration, just before update
                 # logger.info(
@@ -89,12 +138,14 @@ class Engine:
                 #     f'{self.top1_meter_A_n}\t{self.top5_meter_A_n}'
                 # )
 
-
             
     def run(self):
         while self.current_epoch < self.epochs:
             print("train epoch: %d" % (self.current_epoch))
             self.train_epoch()
+            if fleet.worker_index()==0:
+                self.logger.info("epoch {} {}".format(self.current_epoch,self.scheduler.get_lr()))
+            self.scheduler.step()
             self.current_epoch += 1
 
 

Local Rank: 0
I0304 13:21:26.895934 38931 nccl_context.cc:189] init nccl context nranks: 2 local rank: 0 gpu id: 0 ring id: 0
W0304 13:21:27.447440 38931 device_context.cc:362] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.0, Runtime API Version: 10.1
W0304 13:21:27.458093 38931 device_context.cc:372] device: 0, cuDNN Version: 7.6.
backbone:  s3dg
Traceback (most recent call last):
  File "train_paddle.py", line 178, in <module>
    main()
  File "train_paddle.py", line 165, in main
    engine = Engine(args, cfg, local_rank=local_rank)
  File "train_paddle.py", line 52, in __init__
    self.train_loader = self.data_loader_factory.build(vid=True)
  File "/home/users/wuhuachao/RSPNet_paddle/datasets/classification/__init__.py", line 97, in build
    blacklist=self.cfg.get_list('dataset.blacklist'),
  File "/home/users/wuhuachao/RSPNet_paddle/datasets/classification/kinetics.py", line 37, in __init__
    raise Exception(f'No video found in {search_dir}')
Exception: No video found in data/kinetics100/train_video
grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
/usr/local/lib/python3.7/site-packages/paddle/fluid/framework.py:298: UserWarning: You are using GPU version Paddle, but your CUDA device is not set properly. CPU device will be used by default.
  "You are using GPU version Paddle, but your CUDA device is not set properly. CPU device will be used by default."
argument type:  <class 'str'>
Working tree is dirty. Patch:
diff --git a/config/pretrain/moco-train-base.jsonnet b/config/pretrain/moco-train-base.jsonnet
index 252faac..fb9301f 100644
--- a/config/pretrain/moco-train-base.jsonnet
+++ b/config/pretrain/moco-train-base.jsonnet
@@ -10,7 +10,7 @@ local loss_lambda = import "../optimizer/loss_lambda.libsonnet";
         arch: $.arch,
     },
 
-    dataset: kinetics100, // or kinetics100
+    dataset: kinetics100, // or kinetics400
 
     batch_size: 64,
     num_workers: 4,
diff --git a/config/pretrain/s3dg.jsonnet b/config/pretrain/s3dg.jsonnet
index bf6e94a..f93c786 100644
--- a/config/pretrain/s3dg.jsonnet
+++ b/config/pretrain/s3dg.jsonnet
@@ -1,7 +1,7 @@
 local base = import "moco-train-base.jsonnet";
 
 base {
-    batch_size: 64,
+    batch_size: 32,
     num_workers: 4,
 
     arch: 's3dg',
diff --git a/framework/arguments.py b/framework/arguments.py
index 68f2955..883187a 100644
--- a/framework/arguments.py
+++ b/framework/arguments.py
@@ -77,8 +77,13 @@ class Args(TypedArgs):
         if self.experiment_dir is not None:
             self.experiment_dir.mkdir(parents=True, exist_ok=True)
             if not self.ask_for_replacing_older_dir(self.run_dir):
-                raise EnvironmentError(f'Run dir "{self.run_dir}" exists')
-            self.run_dir.mkdir(parents=True, exist_ok=False)
+                print(f'Run dir "{self.run_dir}" exists')
+                #raise EnvironmentError(f'Run dir "{self.run_dir}" exists')
+            else:
+                try:
+                    self.run_dir.mkdir(parents=True, exist_ok=False)
+                except Exception as e:
+                    print(f'Run dir "{self.run_dir}" exists#################')
 
     def make_experiment_dir(self):
         if not self.ask_for_replacing_older_dir(self.experiment_dir):
@@ -92,9 +97,10 @@ class Args(TypedArgs):
         print(
             f'File exists: {dir_to_be_replaced}\nDo you want to remove it and create a new one?'
         )
-        choice = input('Remove older directory? [y]es/[n]o: ')
-
-        if choice in ['y', 'yes']:
-            shutil.rmtree(dir_to_be_replaced)
-            return True
-        return False
+        #choice = input('Remove older directory? [y]es/[n]o: ')
+        shutil.rmtree(dir_to_be_replaced)
+        return True
+        #if choice in ['y', 'yes']:
+        #    shutil.rmtree(dir_to_be_replaced)
+        #    return True
+        #return False
diff --git a/model_paddle.py b/model_paddle.py
index cf7c6a9..338b70f 100644
--- a/model_paddle.py
+++ b/model_paddle.py
@@ -189,15 +189,15 @@ class MoCoDiffLossTwoFc(nn.Layer):
     @paddle.no_grad()
     def _forward_encoder_k(self, im_k):
         # shuffle for making use of BN
-        im_k, idx_unshuffle = self._batch_shuffle_ddp(im_k)
+        #im_k, idx_unshuffle = self._batch_shuffle_ddp(im_k)
 
         k_A, k_M = self.encoder_k(im_k)  # keys: NxC
         # k_A = self.encoder_q(im_k)
         # k_M = self.encoder_q(im_k)
 
         # undo shuffle
-        k_A = self._batch_unshuffle_ddp(k_A, idx_unshuffle)
-        k_M = self._batch_unshuffle_ddp(k_M, idx_unshuffle)
+        #k_A = self._batch_unshuffle_ddp(k_A, idx_unshuffle)
+        #k_M = self._batch_unshuffle_ddp(k_M, idx_unshuffle)
 
         return k_A, k_M
 
@@ -305,6 +305,7 @@ def concat_all_gather(tensor):
 
     tensors_gather = [paddle.ones_like(tensor)
         for _ in range(paddle.distributed.get_world_size())]
+    tensors_gather=[]
     paddle.distributed.all_gather(tensors_gather, tensor)
 
     output = paddle.concat(tensors_gather, axis=0)
diff --git a/models_paddle/__init__.py b/models_paddle/__init__.py
index f9ac301..fa0187a 100644
--- a/models_paddle/__init__.py
+++ b/models_paddle/__init__.py
@@ -19,6 +19,9 @@ def get_model_class(arch):
     elif arch == 'resnet50':
         from .resnet import resnet50
         model_class = resnet50
+    elif arch == 's3dg':
+        from .s3dg import S3D_G
+        model_class = S3D_G
     else:
         raise ValueError('Unknown model architecture "{%s}"' % (arch))
 
diff --git a/pretrain.py b/pretrain.py
index f9feda9..0696980 100644
--- a/pretrain.py
+++ b/pretrain.py
@@ -24,7 +24,7 @@ from framework.utils.checkpoint import CheckpointManager
 from moco import ModelFactory
 from moco.builder_diffspeed_diffloss import Loss
 from utils.moco import replace_moco_k_in_config
-
+import logging
 logger = logging.getLogger(__name__)
 
 
@@ -51,7 +51,14 @@ class Engine:
             A=self.loss_lambda.get_float('A'),
             M=self.loss_lambda.get_float('M')
         )
-
+        
+        #whc add a logger
+        self.logger=logging.getLogger()
+        self.logger.setLevel(logging.INFO)
+        fh=logging.FileHandler("test_log/torch_res.log",mode="w")
+        fh.setLevel(logging.INFO)
+        self.logger.addHandler(fh)
+        
         self.train_loader = self.data_loader_factory.build(vid=True, device=self.device)
 
         self.learning_rate = self.cfg.get_float('optimizer.lr')
@@ -173,7 +180,9 @@ class Engine:
             batch_size = len(clip_q)
             self.top1_meter_A_n.update(acc1_A_n, batch_size)
             self.top5_meter_A_n.update(acc5_A_n, batch_size)
-
+            
+            #whc
+            self.logger.info("whc {} {} {} {} {} {} {} {}".format(self.current_epoch,i,loss.detach().numpy()[0], loss_A.detach().numpy()[0], loss_M.detach().numpy()[0],acc1_A.detach().numpy()[0], acc5_A.detach().numpy()[0], acc1_M.detach().numpy()[0]))
             if i > 0 and i % self.log_interval == 0:
                 # Do logging as late as possible. this will force CUDA sync.
                 # Log numbers from last iteration, just before update
diff --git a/train_paddle.py b/train_paddle.py
index 142f2b7..acaecb3 100644
--- a/train_paddle.py
+++ b/train_paddle.py
@@ -6,14 +6,16 @@ import paddle.distributed.spawn as spawn
 import numpy as np
 import model_paddle as model
 import models.resnet as resnet
-
+import os
 from arguments import Args
 from framework import utils
-
 from datasets.classification import DataLoaderFactoryV3
 from framework.config import get_config, save_config
+import paddle.distributed.fleet.base.role_maker as role_maker
+#from framework.utils.whc_checkpoint import CheckpointManager
 from paddle.distributed import fleet
-
+#whc
+import logging
 
 class Engine:
     def __init__(self, args, cfg, local_rank):
@@ -22,9 +24,28 @@ class Engine:
         self.current_epoch = 0
         # self.batch_size = self.args.train_batch_size
         self.batch_size = cfg.get_int('batch_size')
-        self.batch_size = 64
-
+        #self.batch_size = 64
+        self.local_rank=local_rank
+        self.best_loss = float('inf')
+        self.arch = cfg.get_string('arch')
+        role = role_maker.PaddleCloudRoleMaker(is_collective=True)
         fleet.init(is_collective=True)
+        
+        if fleet.worker_index()==0:
+            #whc add a logger
+            self.logger=logging.getLogger()
+            self.logger.setLevel(logging.INFO)
+            fh=logging.FileHandler("test_log/paddle_res.log",mode="w")
+            fh.setLevel(logging.INFO)
+            self.logger.addHandler(fh)
+            #self.checkpoint = CheckpointManager(
+            #    args.experiment_dir,
+            #    keep_interval=cfg.get_int('checkpoint_interval')
+            #)
+        else:
+            self.checkpoint = None
+
+        #fleet.init(is_collective=True)
         self.model = model.MoCoWrapper(cfg).build_moco_diffloss()
 
         self.data_loader_factory = DataLoaderFactoryV3(cfg)
@@ -37,10 +58,17 @@ class Engine:
             A=self.loss_lambda.get_float('A'),
             M=self.loss_lambda.get_float('M')
         )
-
+        print("####################world_size=",self.args.world_size)
+        if not self.args.no_scale_lr:
+            self.learning_rate = utils.environment.scale_learning_rate(
+                self.learning_rate,
+                self.args.world_size if self.args.world_size==8 else 8,
+                self.batch_size,
+            ) 
+        self.scheduler = paddle.optimizer.lr.CosineAnnealingDecay(learning_rate=self.learning_rate,T_max=self.epochs,eta_min=self.learning_rate/1000)
         self.optimizer = paddle.optimizer.Momentum(
             parameters=self.model.parameters(),
-            learning_rate=self.learning_rate,
+            learning_rate=self.scheduler,
             momentum=cfg.get_float('optimizer.momentum'),
             # dampening=cfg.get_float('optimizer.dampening'),
             weight_decay=cfg.get_float('optimizer.weight_decay'),
@@ -57,6 +85,8 @@ class Engine:
         clip_q = paddle.randn([self.batch_size, 3, 16, 112, 112])
         clip_k = paddle.randn([self.batch_size, 3, 16, 112, 112])
         # for i in range(loop):
+        avg_loss=0
+        nn=0
         for i, (clip_q, clip_k) in enumerate(self.train_loader):
             output, target, ranking_logits, ranking_target = self.model(clip_q, clip_k)
             loss, loss_A, loss_M = self.criterion(output, target, ranking_logits, ranking_target)
@@ -76,10 +106,29 @@ class Engine:
             # acc1_A_n = accuracy(output[1], target, k=1)
             # acc5_A_n = accuracy(output[1], target, k=5)
 
+            #whc get the avg_loss
+            avg_loss += loss.detach().numpy()[0]
+            nn += 1
+
             # if i > 0 and i % self.log_interval == 0:
             if True:
                 print("loss: %f, loss_A: %f, loss_M: %f" % (loss, loss_A, loss_M))
                 print("acc1_A: %f, acc5_A: %f, acc1_M: %f" % (acc1_A, acc5_A, acc1_M))
+                if fleet.worker_index()==0:
+                    self.logger.info("whc {} {} {} {} {} {} {} {}".format(self.current_epoch,i,loss.detach().numpy()[0], loss_A.detach().numpy()[0], loss_M.detach().numpy()[0],acc1_A.detach().numpy()[0], acc5_A.detach().numpy()[0], acc1_M.detach().numpy()[0]))
+
+        if fleet.worker_index()== 0:
+            avg_loss = avg_loss / nn
+            is_best = avg_loss < self.best_loss
+            self.best_loss = min(avg_loss,self.best_loss)
+            if is_best:
+                best_param=os.path.join(str(self.args.experiment_dir),"best_mode.pdparams")
+                best_optim=os.path.join(str(self.args.experiment_dir),"best_optim.pdparams")
+                best_sche=os.path.join(str(self.args.experiment_dir),"best_sche.pdparams")
+                paddle.save(self.model.state_dict(),best_param)
+                paddle.save(self.optimizer.state_dict(),best_optim)
+                paddle.save(self.scheduler.state_dict(),best_sche) 
+ 
                 # Do logging as late as possible. this will force CUDA sync.
                 # Log numbers from last iteration, just before update
                 # logger.info(
@@ -89,12 +138,14 @@ class Engine:
                 #     f'{self.top1_meter_A_n}\t{self.top5_meter_A_n}'
                 # )
 
-
             
     def run(self):
         while self.current_epoch < self.epochs:
             print("train epoch: %d" % (self.current_epoch))
             self.train_epoch()
+            if fleet.worker_index()==0:
+                self.logger.info("epoch {} {}".format(self.current_epoch,self.scheduler.get_lr()))
+            self.scheduler.step()
             self.current_epoch += 1
 
 

Local Rank: 0
backbone:  s3dg
####################world_size= 0
/usr/local/lib/python3.7/site-packages/paddle/fluid/dygraph/parallel.py:424: UserWarning: The program will return to single-card operation. Please check 1, whether you use spawn or fleetrun to start the program. 2, Whether it is a multi-card program. 3, Is the current environment multi-card.
  warnings.warn("The program will return to single-card operation. "
train epoch: 0
Traceback (most recent call last):
  File "train_paddle.py", line 178, in <module>
    main()
  File "train_paddle.py", line 166, in main
    engine.run()
  File "train_paddle.py", line 145, in run
    self.train_epoch()
  File "train_paddle.py", line 90, in train_epoch
    for i, (clip_q, clip_k) in enumerate(self.train_loader):
  File "/usr/local/lib/python3.7/site-packages/paddle/fluid/dataloader/dataloader_iter.py", line 365, in __next__
    return self._reader.read_next_var_list()
SystemError: (Fatal) Blocking queue is killed because the data reader raises an exception.
  [Hint: Expected killed_ != true, but received killed_:1 == true:1.] (at /paddle/paddle/fluid/operators/reader/blocking_queue.h:158)

grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
/usr/local/lib/python3.7/site-packages/paddle/fluid/framework.py:298: UserWarning: You are using GPU version Paddle, but your CUDA device is not set properly. CPU device will be used by default.
  "You are using GPU version Paddle, but your CUDA device is not set properly. CPU device will be used by default."
argument type:  <class 'str'>
Working tree is dirty. Patch:
diff --git a/config/pretrain/moco-train-base.jsonnet b/config/pretrain/moco-train-base.jsonnet
index 252faac..fb9301f 100644
--- a/config/pretrain/moco-train-base.jsonnet
+++ b/config/pretrain/moco-train-base.jsonnet
@@ -10,7 +10,7 @@ local loss_lambda = import "../optimizer/loss_lambda.libsonnet";
         arch: $.arch,
     },
 
-    dataset: kinetics100, // or kinetics100
+    dataset: kinetics100, // or kinetics400
 
     batch_size: 64,
     num_workers: 4,
diff --git a/config/pretrain/s3dg.jsonnet b/config/pretrain/s3dg.jsonnet
index bf6e94a..f93c786 100644
--- a/config/pretrain/s3dg.jsonnet
+++ b/config/pretrain/s3dg.jsonnet
@@ -1,7 +1,7 @@
 local base = import "moco-train-base.jsonnet";
 
 base {
-    batch_size: 64,
+    batch_size: 32,
     num_workers: 4,
 
     arch: 's3dg',
diff --git a/datasets/classification/video.py b/datasets/classification/video.py
index fdd79bf..c61380a 100644
--- a/datasets/classification/video.py
+++ b/datasets/classification/video.py
@@ -73,6 +73,7 @@ class VideoDataset(Dataset):
         clip_frame_indices_list = [self.temporal_transform(frame_indices) for _ in range(self.num_clips_per_sample)]
         # Fetch all frames in one `vr.get_batch` call
         clip_frame_indices = np.concatenate(clip_frame_indices_list)  # [a1, a2, ..., an, b1, b2, ...,bn]
+        decord.bridge.set_bridge('torch')
         clips: torch.Tensor = vr.get_batch(clip_frame_indices)  # [N*T, H, W, C]
         clip_list = clips.chunk(len(clip_frame_indices_list), dim=0)  # List[Tensor[T, H, W, C]]
 
diff --git a/datasets/transforms_video/transforms_tensor.py b/datasets/transforms_video/transforms_tensor.py
index d008074..6a5de4b 100644
--- a/datasets/transforms_video/transforms_tensor.py
+++ b/datasets/transforms_video/transforms_tensor.py
@@ -223,7 +223,9 @@ class SequentialGPUCollateFn:
 
         batch_size = len(clips)
         num_clips = len(clips[0])
+        
         clip_list: List[List[Optional[Tensor]]] = [[None for _ in range(batch_size)] for _ in range(num_clips)]
+        print("=========AAAAAAAAAA===========")
         for batch_index, clip in enumerate(clips):  # batch of clip0, clip1
             for clip_index, clip_tensor in enumerate(clip):
                 clip_tensor = clip_tensor.cuda(device=self.device, non_blocking=True)
diff --git a/framework/arguments.py b/framework/arguments.py
index 68f2955..883187a 100644
--- a/framework/arguments.py
+++ b/framework/arguments.py
@@ -77,8 +77,13 @@ class Args(TypedArgs):
         if self.experiment_dir is not None:
             self.experiment_dir.mkdir(parents=True, exist_ok=True)
             if not self.ask_for_replacing_older_dir(self.run_dir):
-                raise EnvironmentError(f'Run dir "{self.run_dir}" exists')
-            self.run_dir.mkdir(parents=True, exist_ok=False)
+                print(f'Run dir "{self.run_dir}" exists')
+                #raise EnvironmentError(f'Run dir "{self.run_dir}" exists')
+            else:
+                try:
+                    self.run_dir.mkdir(parents=True, exist_ok=False)
+                except Exception as e:
+                    print(f'Run dir "{self.run_dir}" exists#################')
 
     def make_experiment_dir(self):
         if not self.ask_for_replacing_older_dir(self.experiment_dir):
@@ -92,9 +97,10 @@ class Args(TypedArgs):
         print(
             f'File exists: {dir_to_be_replaced}\nDo you want to remove it and create a new one?'
         )
-        choice = input('Remove older directory? [y]es/[n]o: ')
-
-        if choice in ['y', 'yes']:
-            shutil.rmtree(dir_to_be_replaced)
-            return True
-        return False
+        #choice = input('Remove older directory? [y]es/[n]o: ')
+        shutil.rmtree(dir_to_be_replaced)
+        return True
+        #if choice in ['y', 'yes']:
+        #    shutil.rmtree(dir_to_be_replaced)
+        #    return True
+        #return False
diff --git a/model_paddle.py b/model_paddle.py
index cf7c6a9..338b70f 100644
--- a/model_paddle.py
+++ b/model_paddle.py
@@ -189,15 +189,15 @@ class MoCoDiffLossTwoFc(nn.Layer):
     @paddle.no_grad()
     def _forward_encoder_k(self, im_k):
         # shuffle for making use of BN
-        im_k, idx_unshuffle = self._batch_shuffle_ddp(im_k)
+        #im_k, idx_unshuffle = self._batch_shuffle_ddp(im_k)
 
         k_A, k_M = self.encoder_k(im_k)  # keys: NxC
         # k_A = self.encoder_q(im_k)
         # k_M = self.encoder_q(im_k)
 
         # undo shuffle
-        k_A = self._batch_unshuffle_ddp(k_A, idx_unshuffle)
-        k_M = self._batch_unshuffle_ddp(k_M, idx_unshuffle)
+        #k_A = self._batch_unshuffle_ddp(k_A, idx_unshuffle)
+        #k_M = self._batch_unshuffle_ddp(k_M, idx_unshuffle)
 
         return k_A, k_M
 
@@ -305,6 +305,7 @@ def concat_all_gather(tensor):
 
     tensors_gather = [paddle.ones_like(tensor)
         for _ in range(paddle.distributed.get_world_size())]
+    tensors_gather=[]
     paddle.distributed.all_gather(tensors_gather, tensor)
 
     output = paddle.concat(tensors_gather, axis=0)
diff --git a/models_paddle/__init__.py b/models_paddle/__init__.py
index f9ac301..fa0187a 100644
--- a/models_paddle/__init__.py
+++ b/models_paddle/__init__.py
@@ -19,6 +19,9 @@ def get_model_class(arch):
     elif arch == 'resnet50':
         from .resnet import resnet50
         model_class = resnet50
+    elif arch == 's3dg':
+        from .s3dg import S3D_G
+        model_class = S3D_G
     else:
         raise ValueError('Unknown model architecture "{%s}"' % (arch))
 
diff --git a/pretrain.py b/pretrain.py
index f9feda9..0696980 100644
--- a/pretrain.py
+++ b/pretrain.py
@@ -24,7 +24,7 @@ from framework.utils.checkpoint import CheckpointManager
 from moco import ModelFactory
 from moco.builder_diffspeed_diffloss import Loss
 from utils.moco import replace_moco_k_in_config
-
+import logging
 logger = logging.getLogger(__name__)
 
 
@@ -51,7 +51,14 @@ class Engine:
             A=self.loss_lambda.get_float('A'),
             M=self.loss_lambda.get_float('M')
         )
-
+        
+        #whc add a logger
+        self.logger=logging.getLogger()
+        self.logger.setLevel(logging.INFO)
+        fh=logging.FileHandler("test_log/torch_res.log",mode="w")
+        fh.setLevel(logging.INFO)
+        self.logger.addHandler(fh)
+        
         self.train_loader = self.data_loader_factory.build(vid=True, device=self.device)
 
         self.learning_rate = self.cfg.get_float('optimizer.lr')
@@ -173,7 +180,9 @@ class Engine:
             batch_size = len(clip_q)
             self.top1_meter_A_n.update(acc1_A_n, batch_size)
             self.top5_meter_A_n.update(acc5_A_n, batch_size)
-
+            
+            #whc
+            self.logger.info("whc {} {} {} {} {} {} {} {}".format(self.current_epoch,i,loss.detach().numpy()[0], loss_A.detach().numpy()[0], loss_M.detach().numpy()[0],acc1_A.detach().numpy()[0], acc5_A.detach().numpy()[0], acc1_M.detach().numpy()[0]))
             if i > 0 and i % self.log_interval == 0:
                 # Do logging as late as possible. this will force CUDA sync.
                 # Log numbers from last iteration, just before update
diff --git a/train_paddle.py b/train_paddle.py
index 142f2b7..acaecb3 100644
--- a/train_paddle.py
+++ b/train_paddle.py
@@ -6,14 +6,16 @@ import paddle.distributed.spawn as spawn
 import numpy as np
 import model_paddle as model
 import models.resnet as resnet
-
+import os
 from arguments import Args
 from framework import utils
-
 from datasets.classification import DataLoaderFactoryV3
 from framework.config import get_config, save_config
+import paddle.distributed.fleet.base.role_maker as role_maker
+#from framework.utils.whc_checkpoint import CheckpointManager
 from paddle.distributed import fleet
-
+#whc
+import logging
 
 class Engine:
     def __init__(self, args, cfg, local_rank):
@@ -22,9 +24,28 @@ class Engine:
         self.current_epoch = 0
         # self.batch_size = self.args.train_batch_size
         self.batch_size = cfg.get_int('batch_size')
-        self.batch_size = 64
-
+        #self.batch_size = 64
+        self.local_rank=local_rank
+        self.best_loss = float('inf')
+        self.arch = cfg.get_string('arch')
+        role = role_maker.PaddleCloudRoleMaker(is_collective=True)
         fleet.init(is_collective=True)
+        
+        if fleet.worker_index()==0:
+            #whc add a logger
+            self.logger=logging.getLogger()
+            self.logger.setLevel(logging.INFO)
+            fh=logging.FileHandler("test_log/paddle_res.log",mode="w")
+            fh.setLevel(logging.INFO)
+            self.logger.addHandler(fh)
+            #self.checkpoint = CheckpointManager(
+            #    args.experiment_dir,
+            #    keep_interval=cfg.get_int('checkpoint_interval')
+            #)
+        else:
+            self.checkpoint = None
+
+        #fleet.init(is_collective=True)
         self.model = model.MoCoWrapper(cfg).build_moco_diffloss()
 
         self.data_loader_factory = DataLoaderFactoryV3(cfg)
@@ -37,10 +58,17 @@ class Engine:
             A=self.loss_lambda.get_float('A'),
             M=self.loss_lambda.get_float('M')
         )
-
+        print("####################world_size=",self.args.world_size)
+        if not self.args.no_scale_lr:
+            self.learning_rate = utils.environment.scale_learning_rate(
+                self.learning_rate,
+                self.args.world_size if self.args.world_size==8 else 8,
+                self.batch_size,
+            ) 
+        self.scheduler = paddle.optimizer.lr.CosineAnnealingDecay(learning_rate=self.learning_rate,T_max=self.epochs,eta_min=self.learning_rate/1000)
         self.optimizer = paddle.optimizer.Momentum(
             parameters=self.model.parameters(),
-            learning_rate=self.learning_rate,
+            learning_rate=self.scheduler,
             momentum=cfg.get_float('optimizer.momentum'),
             # dampening=cfg.get_float('optimizer.dampening'),
             weight_decay=cfg.get_float('optimizer.weight_decay'),
@@ -57,6 +85,8 @@ class Engine:
         clip_q = paddle.randn([self.batch_size, 3, 16, 112, 112])
         clip_k = paddle.randn([self.batch_size, 3, 16, 112, 112])
         # for i in range(loop):
+        avg_loss=0
+        nn=0
         for i, (clip_q, clip_k) in enumerate(self.train_loader):
             output, target, ranking_logits, ranking_target = self.model(clip_q, clip_k)
             loss, loss_A, loss_M = self.criterion(output, target, ranking_logits, ranking_target)
@@ -76,10 +106,29 @@ class Engine:
             # acc1_A_n = accuracy(output[1], target, k=1)
             # acc5_A_n = accuracy(output[1], target, k=5)
 
+            #whc get the avg_loss
+            avg_loss += loss.detach().numpy()[0]
+            nn += 1
+
             # if i > 0 and i % self.log_interval == 0:
             if True:
                 print("loss: %f, loss_A: %f, loss_M: %f" % (loss, loss_A, loss_M))
                 print("acc1_A: %f, acc5_A: %f, acc1_M: %f" % (acc1_A, acc5_A, acc1_M))
+                if fleet.worker_index()==0:
+                    self.logger.info("whc {} {} {} {} {} {} {} {}".format(self.current_epoch,i,loss.detach().numpy()[0], loss_A.detach().numpy()[0], loss_M.detach().numpy()[0],acc1_A.detach().numpy()[0], acc5_A.detach().numpy()[0], acc1_M.detach().numpy()[0]))
+
+        if fleet.worker_index()== 0:
+            avg_loss = avg_loss / nn
+            is_best = avg_loss < self.best_loss
+            self.best_loss = min(avg_loss,self.best_loss)
+            if is_best:
+                best_param=os.path.join(str(self.args.experiment_dir),"best_mode.pdparams")
+                best_optim=os.path.join(str(self.args.experiment_dir),"best_optim.pdparams")
+                best_sche=os.path.join(str(self.args.experiment_dir),"best_sche.pdparams")
+                paddle.save(self.model.state_dict(),best_param)
+                paddle.save(self.optimizer.state_dict(),best_optim)
+                paddle.save(self.scheduler.state_dict(),best_sche) 
+ 
                 # Do logging as late as possible. this will force CUDA sync.
                 # Log numbers from last iteration, just before update
                 # logger.info(
@@ -89,12 +138,14 @@ class Engine:
                 #     f'{self.top1_meter_A_n}\t{self.top5_meter_A_n}'
                 # )
 
-
             
     def run(self):
         while self.current_epoch < self.epochs:
             print("train epoch: %d" % (self.current_epoch))
             self.train_epoch()
+            if fleet.worker_index()==0:
+                self.logger.info("epoch {} {}".format(self.current_epoch,self.scheduler.get_lr()))
+            self.scheduler.step()
             self.current_epoch += 1
 
 

Local Rank: 0
backbone:  s3dg
####################world_size= 0
/usr/local/lib/python3.7/site-packages/paddle/fluid/dygraph/parallel.py:424: UserWarning: The program will return to single-card operation. Please check 1, whether you use spawn or fleetrun to start the program. 2, Whether it is a multi-card program. 3, Is the current environment multi-card.
  warnings.warn("The program will return to single-card operation. "
train epoch: 0
=========AAAAAAAAAA===========
Traceback (most recent call last):
  File "train_paddle.py", line 178, in <module>
    main()
  File "train_paddle.py", line 166, in main
    engine.run()
  File "train_paddle.py", line 145, in run
    self.train_epoch()
  File "train_paddle.py", line 90, in train_epoch
Exception in thread Thread-1:
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/threading.py", line 917, in _bootstrap_inner
    self.run()
  File "/usr/local/lib/python3.7/threading.py", line 865, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/local/lib/python3.7/site-packages/paddle/fluid/dataloader/dataloader_iter.py", line 347, in _thread_loop
    six.reraise(*sys.exc_info())
  File "/usr/local/lib/python3.7/site-packages/six.py", line 693, in reraise
    raise value
  File "/usr/local/lib/python3.7/site-packages/paddle/fluid/dataloader/dataloader_iter.py", line 317, in _thread_loop
    batch = self._dataset_fetcher.fetch(indices)
  File "/usr/local/lib/python3.7/site-packages/paddle/fluid/dataloader/fetcher.py", line 65, in fetch
    data = self.collate_fn(data)
  File "/home/users/wuhuachao/RSPNet_paddle/datasets/transforms_video/transforms_tensor.py", line 231, in __call__
    clip_tensor = clip_tensor.cuda(device=self.device, non_blocking=True)
  File "/usr/local/lib/python3.7/site-packages/torch/cuda/__init__.py", line 186, in _lazy_init
    _check_driver()
  File "/usr/local/lib/python3.7/site-packages/torch/cuda/__init__.py", line 68, in _check_driver
    http://www.nvidia.com/Download/index.aspx""")
AssertionError: 
Found no NVIDIA driver on your system. Please check that you
have an NVIDIA GPU and installed a driver from
http://www.nvidia.com/Download/index.aspx

    for i, (clip_q, clip_k) in enumerate(self.train_loader):
  File "/usr/local/lib/python3.7/site-packages/paddle/fluid/dataloader/dataloader_iter.py", line 365, in __next__
    return self._reader.read_next_var_list()
SystemError: (Fatal) Blocking queue is killed because the data reader raises an exception.
  [Hint: Expected killed_ != true, but received killed_:1 == true:1.] (at /paddle/paddle/fluid/operators/reader/blocking_queue.h:158)

grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
argument type:  <class 'str'>
Working tree is dirty. Patch:
diff --git a/config/pretrain/moco-train-base.jsonnet b/config/pretrain/moco-train-base.jsonnet
index 252faac..fb9301f 100644
--- a/config/pretrain/moco-train-base.jsonnet
+++ b/config/pretrain/moco-train-base.jsonnet
@@ -10,7 +10,7 @@ local loss_lambda = import "../optimizer/loss_lambda.libsonnet";
         arch: $.arch,
     },
 
-    dataset: kinetics100, // or kinetics100
+    dataset: kinetics100, // or kinetics400
 
     batch_size: 64,
     num_workers: 4,
diff --git a/config/pretrain/s3dg.jsonnet b/config/pretrain/s3dg.jsonnet
index bf6e94a..f93c786 100644
--- a/config/pretrain/s3dg.jsonnet
+++ b/config/pretrain/s3dg.jsonnet
@@ -1,7 +1,7 @@
 local base = import "moco-train-base.jsonnet";
 
 base {
-    batch_size: 64,
+    batch_size: 32,
     num_workers: 4,
 
     arch: 's3dg',
diff --git a/datasets/classification/video.py b/datasets/classification/video.py
index fdd79bf..c61380a 100644
--- a/datasets/classification/video.py
+++ b/datasets/classification/video.py
@@ -73,6 +73,7 @@ class VideoDataset(Dataset):
         clip_frame_indices_list = [self.temporal_transform(frame_indices) for _ in range(self.num_clips_per_sample)]
         # Fetch all frames in one `vr.get_batch` call
         clip_frame_indices = np.concatenate(clip_frame_indices_list)  # [a1, a2, ..., an, b1, b2, ...,bn]
+        decord.bridge.set_bridge('torch')
         clips: torch.Tensor = vr.get_batch(clip_frame_indices)  # [N*T, H, W, C]
         clip_list = clips.chunk(len(clip_frame_indices_list), dim=0)  # List[Tensor[T, H, W, C]]
 
diff --git a/datasets/transforms_video/transforms_tensor.py b/datasets/transforms_video/transforms_tensor.py
index d008074..6a5de4b 100644
--- a/datasets/transforms_video/transforms_tensor.py
+++ b/datasets/transforms_video/transforms_tensor.py
@@ -223,7 +223,9 @@ class SequentialGPUCollateFn:
 
         batch_size = len(clips)
         num_clips = len(clips[0])
+        
         clip_list: List[List[Optional[Tensor]]] = [[None for _ in range(batch_size)] for _ in range(num_clips)]
+        print("=========AAAAAAAAAA===========")
         for batch_index, clip in enumerate(clips):  # batch of clip0, clip1
             for clip_index, clip_tensor in enumerate(clip):
                 clip_tensor = clip_tensor.cuda(device=self.device, non_blocking=True)
diff --git a/framework/arguments.py b/framework/arguments.py
index 68f2955..883187a 100644
--- a/framework/arguments.py
+++ b/framework/arguments.py
@@ -77,8 +77,13 @@ class Args(TypedArgs):
         if self.experiment_dir is not None:
             self.experiment_dir.mkdir(parents=True, exist_ok=True)
             if not self.ask_for_replacing_older_dir(self.run_dir):
-                raise EnvironmentError(f'Run dir "{self.run_dir}" exists')
-            self.run_dir.mkdir(parents=True, exist_ok=False)
+                print(f'Run dir "{self.run_dir}" exists')
+                #raise EnvironmentError(f'Run dir "{self.run_dir}" exists')
+            else:
+                try:
+                    self.run_dir.mkdir(parents=True, exist_ok=False)
+                except Exception as e:
+                    print(f'Run dir "{self.run_dir}" exists#################')
 
     def make_experiment_dir(self):
         if not self.ask_for_replacing_older_dir(self.experiment_dir):
@@ -92,9 +97,10 @@ class Args(TypedArgs):
         print(
             f'File exists: {dir_to_be_replaced}\nDo you want to remove it and create a new one?'
         )
-        choice = input('Remove older directory? [y]es/[n]o: ')
-
-        if choice in ['y', 'yes']:
-            shutil.rmtree(dir_to_be_replaced)
-            return True
-        return False
+        #choice = input('Remove older directory? [y]es/[n]o: ')
+        shutil.rmtree(dir_to_be_replaced)
+        return True
+        #if choice in ['y', 'yes']:
+        #    shutil.rmtree(dir_to_be_replaced)
+        #    return True
+        #return False
diff --git a/model_paddle.py b/model_paddle.py
index cf7c6a9..338b70f 100644
--- a/model_paddle.py
+++ b/model_paddle.py
@@ -189,15 +189,15 @@ class MoCoDiffLossTwoFc(nn.Layer):
     @paddle.no_grad()
     def _forward_encoder_k(self, im_k):
         # shuffle for making use of BN
-        im_k, idx_unshuffle = self._batch_shuffle_ddp(im_k)
+        #im_k, idx_unshuffle = self._batch_shuffle_ddp(im_k)
 
         k_A, k_M = self.encoder_k(im_k)  # keys: NxC
         # k_A = self.encoder_q(im_k)
         # k_M = self.encoder_q(im_k)
 
         # undo shuffle
-        k_A = self._batch_unshuffle_ddp(k_A, idx_unshuffle)
-        k_M = self._batch_unshuffle_ddp(k_M, idx_unshuffle)
+        #k_A = self._batch_unshuffle_ddp(k_A, idx_unshuffle)
+        #k_M = self._batch_unshuffle_ddp(k_M, idx_unshuffle)
 
         return k_A, k_M
 
@@ -305,6 +305,7 @@ def concat_all_gather(tensor):
 
     tensors_gather = [paddle.ones_like(tensor)
         for _ in range(paddle.distributed.get_world_size())]
+    tensors_gather=[]
     paddle.distributed.all_gather(tensors_gather, tensor)
 
     output = paddle.concat(tensors_gather, axis=0)
diff --git a/models_paddle/__init__.py b/models_paddle/__init__.py
index f9ac301..fa0187a 100644
--- a/models_paddle/__init__.py
+++ b/models_paddle/__init__.py
@@ -19,6 +19,9 @@ def get_model_class(arch):
     elif arch == 'resnet50':
         from .resnet import resnet50
         model_class = resnet50
+    elif arch == 's3dg':
+        from .s3dg import S3D_G
+        model_class = S3D_G
     else:
         raise ValueError('Unknown model architecture "{%s}"' % (arch))
 
diff --git a/pretrain.py b/pretrain.py
index f9feda9..0696980 100644
--- a/pretrain.py
+++ b/pretrain.py
@@ -24,7 +24,7 @@ from framework.utils.checkpoint import CheckpointManager
 from moco import ModelFactory
 from moco.builder_diffspeed_diffloss import Loss
 from utils.moco import replace_moco_k_in_config
-
+import logging
 logger = logging.getLogger(__name__)
 
 
@@ -51,7 +51,14 @@ class Engine:
             A=self.loss_lambda.get_float('A'),
             M=self.loss_lambda.get_float('M')
         )
-
+        
+        #whc add a logger
+        self.logger=logging.getLogger()
+        self.logger.setLevel(logging.INFO)
+        fh=logging.FileHandler("test_log/torch_res.log",mode="w")
+        fh.setLevel(logging.INFO)
+        self.logger.addHandler(fh)
+        
         self.train_loader = self.data_loader_factory.build(vid=True, device=self.device)
 
         self.learning_rate = self.cfg.get_float('optimizer.lr')
@@ -173,7 +180,9 @@ class Engine:
             batch_size = len(clip_q)
             self.top1_meter_A_n.update(acc1_A_n, batch_size)
             self.top5_meter_A_n.update(acc5_A_n, batch_size)
-
+            
+            #whc
+            self.logger.info("whc {} {} {} {} {} {} {} {}".format(self.current_epoch,i,loss.detach().numpy()[0], loss_A.detach().numpy()[0], loss_M.detach().numpy()[0],acc1_A.detach().numpy()[0], acc5_A.detach().numpy()[0], acc1_M.detach().numpy()[0]))
             if i > 0 and i % self.log_interval == 0:
                 # Do logging as late as possible. this will force CUDA sync.
                 # Log numbers from last iteration, just before update
diff --git a/train_paddle.py b/train_paddle.py
index 142f2b7..acaecb3 100644
--- a/train_paddle.py
+++ b/train_paddle.py
@@ -6,14 +6,16 @@ import paddle.distributed.spawn as spawn
 import numpy as np
 import model_paddle as model
 import models.resnet as resnet
-
+import os
 from arguments import Args
 from framework import utils
-
 from datasets.classification import DataLoaderFactoryV3
 from framework.config import get_config, save_config
+import paddle.distributed.fleet.base.role_maker as role_maker
+#from framework.utils.whc_checkpoint import CheckpointManager
 from paddle.distributed import fleet
-
+#whc
+import logging
 
 class Engine:
     def __init__(self, args, cfg, local_rank):
@@ -22,9 +24,28 @@ class Engine:
         self.current_epoch = 0
         # self.batch_size = self.args.train_batch_size
         self.batch_size = cfg.get_int('batch_size')
-        self.batch_size = 64
-
+        #self.batch_size = 64
+        self.local_rank=local_rank
+        self.best_loss = float('inf')
+        self.arch = cfg.get_string('arch')
+        role = role_maker.PaddleCloudRoleMaker(is_collective=True)
         fleet.init(is_collective=True)
+        
+        if fleet.worker_index()==0:
+            #whc add a logger
+            self.logger=logging.getLogger()
+            self.logger.setLevel(logging.INFO)
+            fh=logging.FileHandler("test_log/paddle_res.log",mode="w")
+            fh.setLevel(logging.INFO)
+            self.logger.addHandler(fh)
+            #self.checkpoint = CheckpointManager(
+            #    args.experiment_dir,
+            #    keep_interval=cfg.get_int('checkpoint_interval')
+            #)
+        else:
+            self.checkpoint = None
+
+        #fleet.init(is_collective=True)
         self.model = model.MoCoWrapper(cfg).build_moco_diffloss()
 
         self.data_loader_factory = DataLoaderFactoryV3(cfg)
@@ -37,10 +58,17 @@ class Engine:
             A=self.loss_lambda.get_float('A'),
             M=self.loss_lambda.get_float('M')
         )
-
+        print("####################world_size=",self.args.world_size)
+        if not self.args.no_scale_lr:
+            self.learning_rate = utils.environment.scale_learning_rate(
+                self.learning_rate,
+                self.args.world_size if self.args.world_size==8 else 8,
+                self.batch_size,
+            ) 
+        self.scheduler = paddle.optimizer.lr.CosineAnnealingDecay(learning_rate=self.learning_rate,T_max=self.epochs,eta_min=self.learning_rate/1000)
         self.optimizer = paddle.optimizer.Momentum(
             parameters=self.model.parameters(),
-            learning_rate=self.learning_rate,
+            learning_rate=self.scheduler,
             momentum=cfg.get_float('optimizer.momentum'),
             # dampening=cfg.get_float('optimizer.dampening'),
             weight_decay=cfg.get_float('optimizer.weight_decay'),
@@ -57,6 +85,8 @@ class Engine:
         clip_q = paddle.randn([self.batch_size, 3, 16, 112, 112])
         clip_k = paddle.randn([self.batch_size, 3, 16, 112, 112])
         # for i in range(loop):
+        avg_loss=0
+        nn=0
         for i, (clip_q, clip_k) in enumerate(self.train_loader):
             output, target, ranking_logits, ranking_target = self.model(clip_q, clip_k)
             loss, loss_A, loss_M = self.criterion(output, target, ranking_logits, ranking_target)
@@ -76,10 +106,29 @@ class Engine:
             # acc1_A_n = accuracy(output[1], target, k=1)
             # acc5_A_n = accuracy(output[1], target, k=5)
 
+            #whc get the avg_loss
+            avg_loss += loss.detach().numpy()[0]
+            nn += 1
+
             # if i > 0 and i % self.log_interval == 0:
             if True:
                 print("loss: %f, loss_A: %f, loss_M: %f" % (loss, loss_A, loss_M))
                 print("acc1_A: %f, acc5_A: %f, acc1_M: %f" % (acc1_A, acc5_A, acc1_M))
+                if fleet.worker_index()==0:
+                    self.logger.info("whc {} {} {} {} {} {} {} {}".format(self.current_epoch,i,loss.detach().numpy()[0], loss_A.detach().numpy()[0], loss_M.detach().numpy()[0],acc1_A.detach().numpy()[0], acc5_A.detach().numpy()[0], acc1_M.detach().numpy()[0]))
+
+        if fleet.worker_index()== 0:
+            avg_loss = avg_loss / nn
+            is_best = avg_loss < self.best_loss
+            self.best_loss = min(avg_loss,self.best_loss)
+            if is_best:
+                best_param=os.path.join(str(self.args.experiment_dir),"best_mode.pdparams")
+                best_optim=os.path.join(str(self.args.experiment_dir),"best_optim.pdparams")
+                best_sche=os.path.join(str(self.args.experiment_dir),"best_sche.pdparams")
+                paddle.save(self.model.state_dict(),best_param)
+                paddle.save(self.optimizer.state_dict(),best_optim)
+                paddle.save(self.scheduler.state_dict(),best_sche) 
+ 
                 # Do logging as late as possible. this will force CUDA sync.
                 # Log numbers from last iteration, just before update
                 # logger.info(
@@ -89,12 +138,14 @@ class Engine:
                 #     f'{self.top1_meter_A_n}\t{self.top5_meter_A_n}'
                 # )
 
-
             
     def run(self):
         while self.current_epoch < self.epochs:
             print("train epoch: %d" % (self.current_epoch))
             self.train_epoch()
+            if fleet.worker_index()==0:
+                self.logger.info("epoch {} {}".format(self.current_epoch,self.scheduler.get_lr()))
+            self.scheduler.step()
             self.current_epoch += 1
 
 

Local Rank: 0
I0305 10:49:22.880102 26800 nccl_context.cc:189] init nccl context nranks: 8 local rank: 0 gpu id: 0 ring id: 0
W0305 10:49:25.362716 26800 device_context.cc:362] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.0, Runtime API Version: 10.0
W0305 10:49:25.371548 26800 device_context.cc:372] device: 0, cuDNN Version: 7.6.
backbone:  s3dg
####################world_size= 8
train epoch: 0
=========AAAAAAAAAA===========
/usr/local/python378-gcc540/lib/python3.7/site-packages/paddle/nn/layer/norm.py:636: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
=========AAAAAAAAAA===========
=========AAAAAAAAAA===========
Traceback (most recent call last):
  File "train_paddle.py", line 178, in <module>
    main()
  File "train_paddle.py", line 166, in main
    engine.run()
  File "train_paddle.py", line 145, in run
    self.train_epoch()
  File "train_paddle.py", line 91, in train_epoch
    output, target, ranking_logits, ranking_target = self.model(clip_q, clip_k)
  File "/usr/local/python378-gcc540/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py", line 891, in __call__
    outputs = self.forward(*inputs, **kwargs)
  File "/usr/local/python378-gcc540/lib/python3.7/site-packages/paddle/fluid/dygraph/parallel.py", line 489, in forward
    outputs = self._layers(*inputs, **kwargs)
  File "/usr/local/python378-gcc540/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py", line 891, in __call__
    outputs = self.forward(*inputs, **kwargs)
  File "/home/users/wuhuachao/RSPNet_paddle/model_paddle.py", line 253, in forward
    q_A, q_M = self.encoder_q(im_q)  # queries: NxC
  File "/usr/local/python378-gcc540/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py", line 891, in __call__
    outputs = self.forward(*inputs, **kwargs)
  File "/home/users/wuhuachao/RSPNet_paddle/multitask_wrapper_paddle.py", line 126, in forward
    feat = self.encoder.get_feature(x)
  File "/home/users/wuhuachao/RSPNet_paddle/models_paddle/s3dg.py", line 152, in get_feature
    out = self.feature(x)
  File "/usr/local/python378-gcc540/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py", line 891, in __call__
    outputs = self.forward(*inputs, **kwargs)
  File "/usr/local/python378-gcc540/lib/python3.7/site-packages/paddle/fluid/dygraph/container.py", line 86, in forward
    input = layer(input)
  File "/usr/local/python378-gcc540/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py", line 891, in __call__
    outputs = self.forward(*inputs, **kwargs)
  File "/home/users/wuhuachao/RSPNet_paddle/models_paddle/s3dg.py", line 97, in forward
    out_2 = self.branch2(x)
  File "/usr/local/python378-gcc540/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py", line 891, in __call__
    outputs = self.forward(*inputs, **kwargs)
  File "/usr/local/python378-gcc540/lib/python3.7/site-packages/paddle/fluid/dygraph/container.py", line 86, in forward
    input = layer(input)
  File "/usr/local/python378-gcc540/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py", line 891, in __call__
    outputs = self.forward(*inputs, **kwargs)
  File "/home/users/wuhuachao/RSPNet_paddle/models_paddle/s3dg.py", line 31, in forward
    x = self.bn(x)
  File "/usr/local/python378-gcc540/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py", line 891, in __call__
    outputs = self.forward(*inputs, **kwargs)
  File "/usr/local/python378-gcc540/lib/python3.7/site-packages/paddle/nn/layer/norm.py", line 648, in forward
    use_global_stats=self._use_global_stats)
  File "/usr/local/python378-gcc540/lib/python3.7/site-packages/paddle/nn/functional/norm.py", line 198, in batch_norm
    *attrs)
SystemError: (Fatal) Operator batch_norm raises an paddle::memory::allocation::BadAlloc exception.
The exception content is
:ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 24.500244MB memory on GPU 0, available memory is only 13.750000MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 

 (at /paddle/paddle/fluid/memory/allocation/cuda_allocator.cc:69)
. (at /paddle/paddle/fluid/imperative/tracer.cc:172)

grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
argument type:  <class 'str'>
Working tree is dirty. Patch:
diff --git a/config/pretrain/moco-train-base.jsonnet b/config/pretrain/moco-train-base.jsonnet
index 252faac..fb9301f 100644
--- a/config/pretrain/moco-train-base.jsonnet
+++ b/config/pretrain/moco-train-base.jsonnet
@@ -10,7 +10,7 @@ local loss_lambda = import "../optimizer/loss_lambda.libsonnet";
         arch: $.arch,
     },
 
-    dataset: kinetics100, // or kinetics100
+    dataset: kinetics100, // or kinetics400
 
     batch_size: 64,
     num_workers: 4,
diff --git a/config/pretrain/s3dg.jsonnet b/config/pretrain/s3dg.jsonnet
index bf6e94a..71a0a9f 100644
--- a/config/pretrain/s3dg.jsonnet
+++ b/config/pretrain/s3dg.jsonnet
@@ -1,7 +1,7 @@
 local base = import "moco-train-base.jsonnet";
 
 base {
-    batch_size: 64,
+    batch_size: 2,
     num_workers: 4,
 
     arch: 's3dg',
diff --git a/datasets/classification/video.py b/datasets/classification/video.py
index fdd79bf..c61380a 100644
--- a/datasets/classification/video.py
+++ b/datasets/classification/video.py
@@ -73,6 +73,7 @@ class VideoDataset(Dataset):
         clip_frame_indices_list = [self.temporal_transform(frame_indices) for _ in range(self.num_clips_per_sample)]
         # Fetch all frames in one `vr.get_batch` call
         clip_frame_indices = np.concatenate(clip_frame_indices_list)  # [a1, a2, ..., an, b1, b2, ...,bn]
+        decord.bridge.set_bridge('torch')
         clips: torch.Tensor = vr.get_batch(clip_frame_indices)  # [N*T, H, W, C]
         clip_list = clips.chunk(len(clip_frame_indices_list), dim=0)  # List[Tensor[T, H, W, C]]
 
diff --git a/datasets/transforms_video/transforms_tensor.py b/datasets/transforms_video/transforms_tensor.py
index d008074..6a5de4b 100644
--- a/datasets/transforms_video/transforms_tensor.py
+++ b/datasets/transforms_video/transforms_tensor.py
@@ -223,7 +223,9 @@ class SequentialGPUCollateFn:
 
         batch_size = len(clips)
         num_clips = len(clips[0])
+        
         clip_list: List[List[Optional[Tensor]]] = [[None for _ in range(batch_size)] for _ in range(num_clips)]
+        print("=========AAAAAAAAAA===========")
         for batch_index, clip in enumerate(clips):  # batch of clip0, clip1
             for clip_index, clip_tensor in enumerate(clip):
                 clip_tensor = clip_tensor.cuda(device=self.device, non_blocking=True)
diff --git a/framework/arguments.py b/framework/arguments.py
index 68f2955..883187a 100644
--- a/framework/arguments.py
+++ b/framework/arguments.py
@@ -77,8 +77,13 @@ class Args(TypedArgs):
         if self.experiment_dir is not None:
             self.experiment_dir.mkdir(parents=True, exist_ok=True)
             if not self.ask_for_replacing_older_dir(self.run_dir):
-                raise EnvironmentError(f'Run dir "{self.run_dir}" exists')
-            self.run_dir.mkdir(parents=True, exist_ok=False)
+                print(f'Run dir "{self.run_dir}" exists')
+                #raise EnvironmentError(f'Run dir "{self.run_dir}" exists')
+            else:
+                try:
+                    self.run_dir.mkdir(parents=True, exist_ok=False)
+                except Exception as e:
+                    print(f'Run dir "{self.run_dir}" exists#################')
 
     def make_experiment_dir(self):
         if not self.ask_for_replacing_older_dir(self.experiment_dir):
@@ -92,9 +97,10 @@ class Args(TypedArgs):
         print(
             f'File exists: {dir_to_be_replaced}\nDo you want to remove it and create a new one?'
         )
-        choice = input('Remove older directory? [y]es/[n]o: ')
-
-        if choice in ['y', 'yes']:
-            shutil.rmtree(dir_to_be_replaced)
-            return True
-        return False
+        #choice = input('Remove older directory? [y]es/[n]o: ')
+        shutil.rmtree(dir_to_be_replaced)
+        return True
+        #if choice in ['y', 'yes']:
+        #    shutil.rmtree(dir_to_be_replaced)
+        #    return True
+        #return False
diff --git a/model_paddle.py b/model_paddle.py
index cf7c6a9..338b70f 100644
--- a/model_paddle.py
+++ b/model_paddle.py
@@ -189,15 +189,15 @@ class MoCoDiffLossTwoFc(nn.Layer):
     @paddle.no_grad()
     def _forward_encoder_k(self, im_k):
         # shuffle for making use of BN
-        im_k, idx_unshuffle = self._batch_shuffle_ddp(im_k)
+        #im_k, idx_unshuffle = self._batch_shuffle_ddp(im_k)
 
         k_A, k_M = self.encoder_k(im_k)  # keys: NxC
         # k_A = self.encoder_q(im_k)
         # k_M = self.encoder_q(im_k)
 
         # undo shuffle
-        k_A = self._batch_unshuffle_ddp(k_A, idx_unshuffle)
-        k_M = self._batch_unshuffle_ddp(k_M, idx_unshuffle)
+        #k_A = self._batch_unshuffle_ddp(k_A, idx_unshuffle)
+        #k_M = self._batch_unshuffle_ddp(k_M, idx_unshuffle)
 
         return k_A, k_M
 
@@ -305,6 +305,7 @@ def concat_all_gather(tensor):
 
     tensors_gather = [paddle.ones_like(tensor)
         for _ in range(paddle.distributed.get_world_size())]
+    tensors_gather=[]
     paddle.distributed.all_gather(tensors_gather, tensor)
 
     output = paddle.concat(tensors_gather, axis=0)
diff --git a/models_paddle/__init__.py b/models_paddle/__init__.py
index f9ac301..fa0187a 100644
--- a/models_paddle/__init__.py
+++ b/models_paddle/__init__.py
@@ -19,6 +19,9 @@ def get_model_class(arch):
     elif arch == 'resnet50':
         from .resnet import resnet50
         model_class = resnet50
+    elif arch == 's3dg':
+        from .s3dg import S3D_G
+        model_class = S3D_G
     else:
         raise ValueError('Unknown model architecture "{%s}"' % (arch))
 
diff --git a/pretrain.py b/pretrain.py
index f9feda9..0696980 100644
--- a/pretrain.py
+++ b/pretrain.py
@@ -24,7 +24,7 @@ from framework.utils.checkpoint import CheckpointManager
 from moco import ModelFactory
 from moco.builder_diffspeed_diffloss import Loss
 from utils.moco import replace_moco_k_in_config
-
+import logging
 logger = logging.getLogger(__name__)
 
 
@@ -51,7 +51,14 @@ class Engine:
             A=self.loss_lambda.get_float('A'),
             M=self.loss_lambda.get_float('M')
         )
-
+        
+        #whc add a logger
+        self.logger=logging.getLogger()
+        self.logger.setLevel(logging.INFO)
+        fh=logging.FileHandler("test_log/torch_res.log",mode="w")
+        fh.setLevel(logging.INFO)
+        self.logger.addHandler(fh)
+        
         self.train_loader = self.data_loader_factory.build(vid=True, device=self.device)
 
         self.learning_rate = self.cfg.get_float('optimizer.lr')
@@ -173,7 +180,9 @@ class Engine:
             batch_size = len(clip_q)
             self.top1_meter_A_n.update(acc1_A_n, batch_size)
             self.top5_meter_A_n.update(acc5_A_n, batch_size)
-
+            
+            #whc
+            self.logger.info("whc {} {} {} {} {} {} {} {}".format(self.current_epoch,i,loss.detach().numpy()[0], loss_A.detach().numpy()[0], loss_M.detach().numpy()[0],acc1_A.detach().numpy()[0], acc5_A.detach().numpy()[0], acc1_M.detach().numpy()[0]))
             if i > 0 and i % self.log_interval == 0:
                 # Do logging as late as possible. this will force CUDA sync.
                 # Log numbers from last iteration, just before update
diff --git a/train_paddle.py b/train_paddle.py
index 142f2b7..acaecb3 100644
--- a/train_paddle.py
+++ b/train_paddle.py
@@ -6,14 +6,16 @@ import paddle.distributed.spawn as spawn
 import numpy as np
 import model_paddle as model
 import models.resnet as resnet
-
+import os
 from arguments import Args
 from framework import utils
-
 from datasets.classification import DataLoaderFactoryV3
 from framework.config import get_config, save_config
+import paddle.distributed.fleet.base.role_maker as role_maker
+#from framework.utils.whc_checkpoint import CheckpointManager
 from paddle.distributed import fleet
-
+#whc
+import logging
 
 class Engine:
     def __init__(self, args, cfg, local_rank):
@@ -22,9 +24,28 @@ class Engine:
         self.current_epoch = 0
         # self.batch_size = self.args.train_batch_size
         self.batch_size = cfg.get_int('batch_size')
-        self.batch_size = 64
-
+        #self.batch_size = 64
+        self.local_rank=local_rank
+        self.best_loss = float('inf')
+        self.arch = cfg.get_string('arch')
+        role = role_maker.PaddleCloudRoleMaker(is_collective=True)
         fleet.init(is_collective=True)
+        
+        if fleet.worker_index()==0:
+            #whc add a logger
+            self.logger=logging.getLogger()
+            self.logger.setLevel(logging.INFO)
+            fh=logging.FileHandler("test_log/paddle_res.log",mode="w")
+            fh.setLevel(logging.INFO)
+            self.logger.addHandler(fh)
+            #self.checkpoint = CheckpointManager(
+            #    args.experiment_dir,
+            #    keep_interval=cfg.get_int('checkpoint_interval')
+            #)
+        else:
+            self.checkpoint = None
+
+        #fleet.init(is_collective=True)
         self.model = model.MoCoWrapper(cfg).build_moco_diffloss()
 
         self.data_loader_factory = DataLoaderFactoryV3(cfg)
@@ -37,10 +58,17 @@ class Engine:
             A=self.loss_lambda.get_float('A'),
             M=self.loss_lambda.get_float('M')
         )
-
+        print("####################world_size=",self.args.world_size)
+        if not self.args.no_scale_lr:
+            self.learning_rate = utils.environment.scale_learning_rate(
+                self.learning_rate,
+                self.args.world_size if self.args.world_size==8 else 8,
+                self.batch_size,
+            ) 
+        self.scheduler = paddle.optimizer.lr.CosineAnnealingDecay(learning_rate=self.learning_rate,T_max=self.epochs,eta_min=self.learning_rate/1000)
         self.optimizer = paddle.optimizer.Momentum(
             parameters=self.model.parameters(),
-            learning_rate=self.learning_rate,
+            learning_rate=self.scheduler,
             momentum=cfg.get_float('optimizer.momentum'),
             # dampening=cfg.get_float('optimizer.dampening'),
             weight_decay=cfg.get_float('optimizer.weight_decay'),
@@ -57,6 +85,8 @@ class Engine:
         clip_q = paddle.randn([self.batch_size, 3, 16, 112, 112])
         clip_k = paddle.randn([self.batch_size, 3, 16, 112, 112])
         # for i in range(loop):
+        avg_loss=0
+        nn=0
         for i, (clip_q, clip_k) in enumerate(self.train_loader):
             output, target, ranking_logits, ranking_target = self.model(clip_q, clip_k)
             loss, loss_A, loss_M = self.criterion(output, target, ranking_logits, ranking_target)
@@ -76,10 +106,29 @@ class Engine:
             # acc1_A_n = accuracy(output[1], target, k=1)
             # acc5_A_n = accuracy(output[1], target, k=5)
 
+            #whc get the avg_loss
+            avg_loss += loss.detach().numpy()[0]
+            nn += 1
+
             # if i > 0 and i % self.log_interval == 0:
             if True:
                 print("loss: %f, loss_A: %f, loss_M: %f" % (loss, loss_A, loss_M))
                 print("acc1_A: %f, acc5_A: %f, acc1_M: %f" % (acc1_A, acc5_A, acc1_M))
+                if fleet.worker_index()==0:
+                    self.logger.info("whc {} {} {} {} {} {} {} {}".format(self.current_epoch,i,loss.detach().numpy()[0], loss_A.detach().numpy()[0], loss_M.detach().numpy()[0],acc1_A.detach().numpy()[0], acc5_A.detach().numpy()[0], acc1_M.detach().numpy()[0]))
+
+        if fleet.worker_index()== 0:
+            avg_loss = avg_loss / nn
+            is_best = avg_loss < self.best_loss
+            self.best_loss = min(avg_loss,self.best_loss)
+            if is_best:
+                best_param=os.path.join(str(self.args.experiment_dir),"best_mode.pdparams")
+                best_optim=os.path.join(str(self.args.experiment_dir),"best_optim.pdparams")
+                best_sche=os.path.join(str(self.args.experiment_dir),"best_sche.pdparams")
+                paddle.save(self.model.state_dict(),best_param)
+                paddle.save(self.optimizer.state_dict(),best_optim)
+                paddle.save(self.scheduler.state_dict(),best_sche) 
+ 
                 # Do logging as late as possible. this will force CUDA sync.
                 # Log numbers from last iteration, just before update
                 # logger.info(
@@ -89,12 +138,14 @@ class Engine:
                 #     f'{self.top1_meter_A_n}\t{self.top5_meter_A_n}'
                 # )
 
-
             
     def run(self):
         while self.current_epoch < self.epochs:
             print("train epoch: %d" % (self.current_epoch))
             self.train_epoch()
+            if fleet.worker_index()==0:
+                self.logger.info("epoch {} {}".format(self.current_epoch,self.scheduler.get_lr()))
+            self.scheduler.step()
             self.current_epoch += 1
 
 

Local Rank: 0
W0305 10:51:33.461419 28211 nccl_context.cc:142] Socket connect worker 127.0.0.1:31745 failed, try again after 3 seconds.
I0305 10:51:36.462219 28211 nccl_context.cc:189] init nccl context nranks: 8 local rank: 0 gpu id: 0 ring id: 0
W0305 10:51:39.006708 28211 device_context.cc:362] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.0, Runtime API Version: 10.0
W0305 10:51:39.012511 28211 device_context.cc:372] device: 0, cuDNN Version: 7.6.
backbone:  s3dg
####################world_size= 8
train epoch: 0
=========AAAAAAAAAA===========
=========AAAAAAAAAA===========
=========AAAAAAAAAA===========
/usr/local/python378-gcc540/lib/python3.7/site-packages/paddle/nn/layer/norm.py:636: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
=========AAAAAAAAAA===========
=========AAAAAAAAAA===========
=========AAAAAAAAAA===========
[128, 16384]
loss: 0.295690, loss_A: 0.295690, loss_M: 0.000000
acc1_A: 1.000000, acc5_A: 1.000000, acc1_M: 1.000000
=========AAAAAAAAAA===========
[128, 16384]
loss: 7.337997, loss_A: 7.337997, loss_M: 0.000000
acc1_A: 0.000000, acc5_A: 1.000000, acc1_M: 1.000000
=========AAAAAAAAAA===========
[128, 16384]
loss: 9.784931, loss_A: 6.901491, loss_M: 2.883440
acc1_A: 0.500000, acc5_A: 0.500000, acc1_M: 0.000000
Traceback (most recent call last):
  File "train_paddle.py", line 178, in <module>
    main()
  File "train_paddle.py", line 166, in main
    engine.run()
  File "train_paddle.py", line 145, in run
    self.train_epoch()
  File "train_paddle.py", line 91, in train_epoch
    output, target, ranking_logits, ranking_target = self.model(clip_q, clip_k)
  File "/usr/local/python378-gcc540/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py", line 891, in __call__
    outputs = self.forward(*inputs, **kwargs)
  File "/usr/local/python378-gcc540/lib/python3.7/site-packages/paddle/fluid/dygraph/parallel.py", line 489, in forward
    outputs = self._layers(*inputs, **kwargs)
  File "/usr/local/python378-gcc540/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py", line 891, in __call__
    outputs = self.forward(*inputs, **kwargs)
  File "/home/users/wuhuachao/RSPNet_paddle/model_paddle.py", line 244, in forward
    self._momentum_update_key_encoder()  # update the key encoder
  File "<decorator-gen-227>", line 2, in _momentum_update_key_encoder
  File "/usr/local/python378-gcc540/lib/python3.7/site-packages/paddle/fluid/dygraph/base.py", line 315, in _decorate_function
    return func(*args, **kwargs)
  File "/home/users/wuhuachao/RSPNet_paddle/model_paddle.py", line 119, in _momentum_update_key_encoder
    param_k.set_value(param_k * self.m + param_q * (1. - self.m))
  File "<decorator-gen-113>", line 2, in set_value
  File "/usr/local/python378-gcc540/lib/python3.7/site-packages/paddle/fluid/wrapped_decorator.py", line 25, in __impl__
    return wrapped_func(*args, **kwargs)
  File "/usr/local/python378-gcc540/lib/python3.7/site-packages/paddle/fluid/framework.py", line 225, in __impl__
    return func(*args, **kwargs)
  File "/usr/local/python378-gcc540/lib/python3.7/site-packages/paddle/fluid/dygraph/varbase_patch_methods.py", line 119, in set_value
    value_np = value.numpy()
KeyboardInterrupt
